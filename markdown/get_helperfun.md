get\_helperfun.R
================
Dean Koch
2020-12-18

**Mitacs UYRW project**

**get\_helperfun**: helper functions for the scripts in the UYRW\_data
repository

This script is meant to be sourced by all other scripts in the
repository. It defines some helper functions, directories for local
storage, and some R code that loads commonly used datasets if they are
detected in the data directory.

## libraries

These CRAN packages are quite useful, and are required by some of the
scripts in the repository. If any of these are not already installed on
your machine, run `install.packages(...)` to get them.
[`raster`](https://rspatial.org/raster/) handles raster data such as
GeoTIFFs

``` r
library(raster)
```

[`sf`](https://r-spatial.github.io/sf/) handles GIS data such as ESRI
shapefiles

``` r
library(sf)
```

[`ggplot2`](https://ggplot2.tidyverse.org/) popular graphics package
with high-level abstraction

``` r
library(ggplot2)  
```

[`tmap`](https://github.com/mtennekes/tmap) constructs pretty thematic
map graphics

``` r
library(tmap)
```

[`dplyr`](https://dplyr.tidyverse.org/R) tidyverse-style manipulation of
tabular data

``` r
library(dplyr)
```

[‘RSQLite’](https://www.r-project.org/nosvn/pandoc/RSQLite.html)
connects to SQLite databases

``` r
library(RSQLite)
```

[‘data.table’](https://cran.r-project.org/web/packages/data.table/index.html)
efficiently load large I/O files.

``` r
library(data.table)
```

## project data

To avoid downloading things over and over again, we’ll use a permanent
storage location on disk (“/data”). This is where we store large data
files and R object binaries, which are not suitable for git.

The `if(!file.exists(...))` conditionals preceding each code chunk
indicate which files will be written in that section. If the files are
detected in the local data storage directory, then that code chunk can
be skipped (to avoid redundant downloads, *etc*), and the files are
loaded from disk instead.

We start by defining a project directory tree

``` r
# 'graphics', 'markdown', 'data' are top level directories in the RStudio project folder
graphics.dir = 'graphics'
markdown.dir = 'markdown'
data.dir = 'data'

# subdirectories of `data` contain source files and their processed output 
src.subdir = 'data/source'
out.subdir = 'data/prepared'
```

Define a helper function for creating folders then create project
folders as needed

``` r
my_dir = function(path) { if(!dir.exists(path)) {dir.create(path, recursive=TRUE)} }
lapply(here(c(data.dir, src.subdir, out.subdir, graphics.dir, markdown.dir)), my_dir)
```

    ## [[1]]
    ## NULL
    ## 
    ## [[2]]
    ## NULL
    ## 
    ## [[3]]
    ## NULL
    ## 
    ## [[4]]
    ## NULL
    ## 
    ## [[5]]
    ## NULL

NAs are represented in the GeoTiff by an integer – usually something
large and negative. The default value for this integer when using
`raster::writeRaster` is so large that it can cause an integer overflow
error in one of the python modules used by QSWAT+ (at least on my 64bit
machine). We instead use the value recommended in the QSWAT+ manual:

``` r
tif.na.val = -32767
```

This project will generate many files. To keep track of everything, each
script gets a CSV table documenting every file that it creates: its file
path, its type, and a short description of the contents. This function
handles the construction of the table. To call up the table for a
specific script, simply use `my_metadata(script.name)`.

``` r
# creates and/or adds to a data frame of metadata documenting a given script, and (optionally) writes it to disk as a CSV file 
my_metadata = function(script.name, entries.list=NA, overwrite=FALSE, use.file=TRUE, data.dir='data')
{
  # ARGUMENTS:
  #
  # `script.name` is a string indicating the filename (without the .R extension) of the R script to document.
  # `entries.list` is a list of character vectors, whose entries are named: 'name', 'file', 'type', and 'description'.
  # `data.dir` is the subdirectory of the project folder in which to write the CSV file: /data/`script.name`_metadata.csv 
  # `use.file` is a boolean indicating whether to read/write the CSV file
  # `overwrite` allows an existing CSV file to be modified 
  #
  # RETURN VALUE:
  #
  # a data frame containing a table of file info, combining the elements of `entries.list` with data from the CSV corresponding
  # to the script name (if it exists)
  #
  # BEHAVIOUR: 
  #
  # With use.file==FALSE, the function simply returns `entries.list`, reshaped as a data.frame. 
  #
  # With use.file==TRUE, the function looks for an existing CSV file, reads it, and combines it with the elements of `entries.list`.
  # New entries take precedence: ie. any element of `entries.list` whose 'name' field matches an existing row in the CSV will replace
  # that row. Elements with names not appearing the CSV are added to the top of the table in the order they appear in `entries.list`.
  #
  # Existing CSV files are never modified, unless `use.file` and `overwrite` are both TRUE, in which case the CSV is overwritten with
  # the modified data frame. If the CSV file does not already exist on disk, it will be created. The default `entries.list==NA`, 
  # combined with `overwrite=TRUE` and `use.file=TRUE` will overwrite the CSV with a default placeholder - a table containing only a
  # single row, which describes the CSV file itself.
  
  # define the CSV filename, and define a flag that will indicate to wipe it replace with a default if requested
  csv.relpath = file.path(data.dir, paste0(script.name, '_metadata.csv'))
  csv.wipe = FALSE
  
  # create the directory if necessary
  my_dir(dirname(here(csv.relpath)))
  
  # prepare the default one-row data.frame 
  entry.names = c('name', 'file', 'type', 'description')
  entry.default = c(name='metadata', file=csv.relpath, type='CSV', description=paste0('list files of files written by ', script.name, '.R'))
  
  # parse `entries.list` to check for wrong syntax or NA input
  if(!is.list(entries.list))
  {
    # if `entries.list` is not a list, it must be a single NA. Stop if it's a vector
    if(length(entries.list) > 1)
    {
      stop('entries.list must be either a list or (length-1) NA value')
      
    } else {
      
      # Catch non-NA, non-list input 
      if(!is.na(entries.list))
      {
        stop('entries.list must be either a list or (length-1) NA value')
      } 
    }
    
    # Recursive call to generate the data frame with default entry
    input.df = my_metadata(script.name, entries.list=list(entry.default), use.file=FALSE)
    
    # set the flag to overwrite with default one-row table, if requested
    if(overwrite) {csv.wipe = TRUE}
    
  } else {
    
    # entries.list is a list. Check for non-character vector inputs
    if(!all(sapply(entries.list, is.character)))
    {
      stop('all elements of entries.list must be character vectors')
    }
    
    # halt on incorrectly named vectors
    if(!all(sapply(entries.list, function(entry) all(names(entry)==entry.names))))
    {
      stop(paste0('each element of entries.list must be a named vector with names: ', paste(entry.names, collapse=', ')))
    }
    
    # entries.list is valid input. Construct the data frame
    input.df = data.frame(do.call(rbind, entries.list), row.names='name')
  }
  
  # data.frame() appears to ignore row.names when nrows==1. Detect this and fix it
  if(!is.null(input.df$name))
  {
    rownames(input.df) = input.df$name
    input.df = input.df[,-which(names(input.df)=='name')]
  }
  
  # if not reading a CSV from disk, we're finished
  if(!use.file)
  {
    return(input.df)
    
  } else {
    
    # create a second data frame to store any data from the csv on disk
    csv.df = my_metadata(script.name, entries.list=list(entry.default), use.file=FALSE)
    
    # look for the file on disk and load it if it exists
    if(file.exists(here(csv.relpath)) & !csv.wipe)
    {
      # load the csv data 
      csv.df = read.csv(here(csv.relpath), header=TRUE, row.names=1)
    }
    
    # identify any entries in csv.df with names matched in entries.list, update them, delete those rows from input.df
    names.updating = rownames(csv.df)[rownames(csv.df) %in% rownames(input.df)]
    csv.df[names.updating,] = input.df[names.updating,]
    input.df = input.df[!(rownames(input.df) %in% names.updating),]
    
    # merge the two data frames
    output.df = rbind(input.df, csv.df)
    if(overwrite)
    {
      # CSV file is written to disk
      print(paste('writing to', csv.relpath))
      write.csv(output.df, here(csv.relpath))
      return(output.df)
      
    } 
    
    return(output.df)
    
  }
}
```

My R scripts are commented using a roxygen2 syntax that is interpretable
by `rmarkdown`. This convenience function renders the markdown file for
a given R script and writes to a file of the same name (but with a .md
extension).

``` r
my_markdown = function(script.name, script.dir='R', markdown.dir='markdown')
{
  # ARGUMENTS:
  #
  # `script.name` is a string indicating the filename (without the .R extension) of the R script to document.
  # `script.dir` is a string indicating which folder in the project directory contains the R script.
  # `markdown.dir` is a string indicating a folder in the project directory to write the output file.
  #
  # RETURN VALUE:
  #
  # (null)
  #
  # BEHAVIOUR: 
  #
  # Writes the file <project directory>/<script.dir>/<script.name>.R, overwriting without warning
  
  # set up in/out files
  path.input = here(script.dir, paste0(script.name, '.R'))
  path.output = here(file.path(markdown.dir, paste0(script.name, '.md')))
  
  # note: run_pandoc=FALSE appears to cause output_dir to be ignored. So this call also generates an html file
  paste('rendering markdown file', path.output, 'from the R script', path.input)
  rmarkdown::render(path.input, clean=TRUE, output_file=path.output)
}
```

This function is used in `get_weatherstations` to parse the time series
dates

``` r
# convert start/end year columns from GHCN data to a single (string) column for each "element"
my_ghcnd_reshape = function(idval, elemval)
{
  # query this combination of station id and element string in the full GHCND table
  idx.row = (ghcnd.df$id == idval) & (ghcnd.df$element == elemval)
  
  # if it exists, return a string of form "start-end", otherwise NA
  return(ifelse(!any(idx.row), NA, paste(ghcnd.df[idx.row, c('first_year', 'last_year')], collapse='-')))
}
```

This function is used by `get_meteo` to extract data from Ben Livneh’s
meteorological reconstruction dataset (NetCDF format), returning R data
frames and `raster`/`sf` objects

``` r
my_livneh_reader = function(nc, perim, buff=0)
{
  # Opens a Livneh meteorological data file and returns a list of R objects
  #
  # ARGUMENTS:
  #
  # `nc`, character vector path to the input NetCDF file (.nc or .bz2, see BEHAVIOUR below)
  # `perim`, a projected sf geometry delineating the perimeter of the area to fetch data
  # `buff`, numeric (metres) distance to extend the perimeter outwards
  #
  # RETURN VALUE:
  #
  # a named list of two lists:
  #
  # `tab`, a named list of five data frames:
  #   `coords`, the geographical coordinates (lat/long) of each grid point imported; and
  #   `pcp`, `tmax`, `tmin`, `wnd`; the daily values for each variable
  #
  # `spat`, a named list of five R objects
  #   `pts`, an sf object locating each grid point (in same projection as `perim`)
  #   `pcp`, `tmax`, `tmin`, `wnd`; rasterstacks with the daily values of each variable
  #
  # BEHAVIOUR: 
  #
  # If the input file path extension is 'bz2', the data are imported by decompressing to
  # a temporary file, which gets deleted at the end (`raster` does not seem to support
  # on-the-fly decompression)
  #
  # Argument `buff` allows inclusion of grid points lying adjacent (and near) the watershed. 
  #
  # Rows of the daily variables tables correspond to days of the month (1st row = 1st day,
  # etc). Each grid location gets its own column, with coordinates stored as rows in the
  # `coords` table (eg the nth row of `coords` gives the coordinates for the precipitation
  # time series in the nth column of `tab[['prec']]`.
  
  # handle bzipped input files via temporary decompressed copy
  is.compressed = substr(nc, nchar(nc)-3, nchar(nc)) == '.bz2'
  if(is.compressed)
  {
    nc.tempfile = tempfile(fileext='.nc')
    print(paste('decompressing', basename(nc), 'to temporary file,', basename(nc.tempfile)))
    nc.raw = memDecompress(readBin(nc, raw(), file.info(nc)$size))
    writeBin(nc.raw, nc.tempfile)
    nc = nc.tempfile
  }

  # define the variable names to extract
  livneh.vars = c(pcp='Prec', tmax='Tmax', tmin='Tmin', wnd='wind')
  
  # open NetCDF as raster (1st band, 1st variable) to extract grid data
  nc.r = raster(nc, varname=livneh.vars[1], band=1)
  
  # mask/crop to `perim` after applying `buff` metres of padding
  perim.t = as(st_transform(st_buffer(perim, buff), crs=st_crs(nc.r)), 'Spatial')
  nc.r.clip = mask(crop(nc.r, perim.t), perim.t)
  idx.na = values(is.na(nc.r.clip))
  
  # build the coordinates table
  coords.tab = data.frame(coordinates(nc.r.clip)) %>% filter(!idx.na)
  n.spatpts = nrow(coords.tab)
  colnames(coords.tab) = c('long', 'lat')
  rownames(coords.tab) = paste0('grid', 1:n.spatpts)
  
  # build the output `pts` sf object
  geopts.sf = st_as_sf(coords.tab, coords=colnames(coords.tab), crs=st_crs(nc.r))
  pts.sf = st_transform(cbind(geopts.sf, data.frame(name=rownames(coords.tab))), st_crs(perim))
  
  # initialize the other output tables
  n.days = nbands(nc.r)
  template.df = data.frame(matrix(NA, n.days, n.spatpts))
  colnames(template.df) = rownames(coords.tab)
  climdata.list = lapply(livneh.vars, function(x) template.df)
  
  # initialize rasterstack list
  rbrick.list = vector(mode='list', length=length(livneh.vars))
  names(rbrick.list) = names(livneh.vars)
  
  # loop to fill these tables and copy rasterstacks
  for(varname in names(livneh.vars))
  {
    rbrick.list[[varname]] = mask(crop(stack(nc, varname=livneh.vars[varname]), perim.t), perim.t)
    climdata.list[[varname]][] = t(values(rbrick.list[[varname]])[!idx.na, ])
  }
  
  # bundle everything into a list and finish, tidying up tempfiles as needed
  if(is.compressed) {unlink(nc.tempfile)}
  return(list(tab=c(list(coords=coords.tab), climdata.list), spat=c(list(pts=pts.sf), rbrick.list)))
  
}
```

This function is used by `get_meteo` to extract data from the ORNL
Daymet meteorological reconstruction dataset (NetCDF format), returning
R data frames and `raster`/`sf` objects It is very similar to
`my_livneh_reader`, except that the Daymet files are not bzipped, and
different meteorological variables are stored in different files.

In the current CRAN version of the `daymetr` package (v1.4), there is a
bug related to a mislabeling of projection information by the web
coverage service (WCS) at ORNL’s DAAC. Read about it in the bug reports
[here](https://github.com/bluegreen-labs/daymetr/issues/40),
[here](https://github.com/ropensci/FedData/issues/50), and
[here](https://github.com/bluegreen-labs/daymetr/issues/36)).

We fix this by manually rewriting the proj4 string after it is loaded
into R. Unfortunately this results in many warnings about invalid CRS
definitions (these can be safely ignored)

``` r
my_daymet_reader = function(nc, perim, buff=0)
{
  # Opens a (set of) Daymet meteorological data file(s) and returns a list of R objects
  #
  # ARGUMENTS:
  #
  # `nc`, named character vector of path(s) to the input NetCDF file(s). See below
  # `perim`, a projected sf geometry delineating the perimeter of the area to fetch data
  # `buff`, numeric (metres) distance to extend the perimeter outwards
  #
  # RETURN VALUE:
  #
  # a named list of two lists, whose lengths depend on the number input NetCDF files (`nc`).
  # A function typical call will provide the paths to the `prcp`, `tmin`, `tmax`, and `srad`
  # files (named using these strings, or similar). 
  #
  # `tab`, a named list of `length(nc)+1` data frames:
  #   `coords`, the geographical coordinates (lat/long) of each grid point imported; and
  #   named entries for each file in `nc`, containing the table of daily values for that
  #   variable.
  #
  # `spat`, a named list of `length(nc)+1` R objects
  #   `pts`, an sf object locating each grid point (in same projection as `perim`); and
  #   named entries for each file in `nc`, containing rasterstacks for each variable
  #
  # BEHAVIOUR: 
  #
  # Argument `buff` allows inclusion of grid points lying adjacent (and near) the watershed. 
  #
  # Rows of the daily variables tables correspond to days of the month (1st row = 1st day,
  # etc). Each grid location gets its own column, with coordinates stored as rows in the
  # `coords` table (eg if `nc` contains the entry `prcp` giving the path to a precipitation
  # data file, the nth row of `coords` gives the coordinates for the precipitation time series
  # in the nth column of `tab[['prcp']]`.
  #
  # The names of the data entries of output lists `tab` and `spat` are copied from the names
  # provided for the `nc` input. If this argument is unnamed, the function assigns them based
  # on the filenames provided.
  
  # warn if the `nc` paths are not named and auto-generate
  if(any(is.null(names(nc))))
  {
    in.vars = substr(basename(nc), 1, 4)
    print(paste('Warning: no name(s) supplied for', paste(basename(nc), sep=', ')))
    print(paste('using name(s)', paste(in.vars, sep=', ')))
    names(nc) = in.vars
    
  } else {
    
   in.vars = names(nc)
   
  }
  
  # open NetCDF as raster (1st band) to extract grid data
  nc.r = raster(nc[[1]], band=1)
  
  # overwrite proj4 string with corrected version (in units of 'km', instead of 'm')
  daymet.proj4 = '+proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +units=km +datum=WGS84 +no_defs'
  projection(nc.r) = daymet.proj4
  
  # mask/crop to `perim` after applying `buff` metres of padding
  perim.t = as(st_transform(st_buffer(perim, buff), crs=st_crs(nc.r)), 'Spatial')
  nc.r.clip = mask(crop(nc.r, perim.t), perim.t)
  idx.na = values(is.na(nc.r.clip))
  
  
  # build the output `pts` sf object
  coords.lcc.tab = data.frame(coordinates(nc.r.clip)) %>% filter(!idx.na)
  n.spatpts = nrow(coords.lcc.tab)
  colnames(coords.lcc.tab) = c('long', 'lat')
  rownames(coords.lcc.tab) = paste0('grid', 1:n.spatpts)
  geopts.sf = st_as_sf(coords.lcc.tab, coords=colnames(coords.lcc.tab), crs=st_crs(nc.r))
  pts.sf = st_transform(cbind(geopts.sf, data.frame(name=rownames(coords.lcc.tab))), st_crs(perim))
  
  # build the lat/long coordinates table
  coords.tab = st_coordinates(st_transform(geopts.sf, crs=4326))
  rownames(coords.tab) = rownames(coords.lcc.tab)

  # initialize the other output tables
  n.days = nbands(nc.r)
  template.df = data.frame(matrix(NA, n.days, n.spatpts))
  colnames(template.df) = rownames(coords.tab)
  climdata.list = lapply(nc, function(x) template.df)
  
  # initialize rasterstack list
  rbrick.list = vector(mode='list', length=length(nc))
  names(rbrick.list) = in.vars
  
  # loop to fill these tables and copy rasterstacks
  for(varname in in.vars)
  {
    # load and fix projection CRS
    nc.in = stack(nc[varname])
    projection(nc.in) = daymet.proj4
    rbrick.list[[varname]] = mask(crop(nc.in, perim.t), perim.t)
    climdata.list[[varname]][] = t(values(rbrick.list[[varname]])[!idx.na, ])
  }
  
  # bundle everything into a list and finish
  return(list(tab=c(list(coords=coords.tab), climdata.list), spat=c(list(pts=pts.sf), rbrick.list)))
  
}
```

This is a kludge combining various `FedData` functions with some of my
own code, in order to import STATSGO2 data and produce output similar to
FedData::get\_ssurgo. It uses data from
[NRCS](https://nrcs.app.box.com/v/soils), which is aggregated at the
state level. Instead of a `template` or SSA code argument, it takes a
state code, such as ‘mt’. Note that not all states are available at this
time (see here for the list)

``` r
my_get_statsgo = function(raw.dir, state, extraction.dir, label='UYRW')
{
  # raw.dir = path of the files extracted from the NRCS zip (this dir should have subdirs 'tabular', 'spatial') 
  # state = 2-letter state code, eg. 'MT' for Montana (case insensitive)
  # extraction.dir = absolute path of destination folder for CSV and shapefile data
  
  # get tables headers (these are not exported to user namespace!)
  tablesHeaders = getFromNamespace('tablesHeaders', 'FedData')
  tablesHeaders$component
  
  # this call adjusted to use STATSGO2 syntax for filenames
  mapunits <- sf::read_sf(paste0(raw.dir, "/spatial"), layer = paste0("gsmsoilmu_a_", tolower(state))) %>%
    sf::st_make_valid() %>%
    dplyr::group_by(AREASYMBOL, SPATIALVER, MUSYM, MUKEY) %>%
    dplyr::summarise()

  # same here
    if (.Platform$OS.type == "windows") 
    {
      files <- list.files(paste0(raw.dir, "/tabular"),full.names = T)
      tablesData <- lapply(files, function(file) {tryCatch(return(utils::read.delim(file, header = F, sep = "|", stringsAsFactors = F)), error = function(e) {return(NULL)})})
      names(tablesData) <- basename(files)
      tablesData <- tablesData[!sapply(tablesData, is.null)]
      
    } else {
      
      files <- list.files(paste0(raw.dir, "/tabular"), full.names = T)
      tablesData <- lapply(files, function(file) { tryCatch(return(utils::read.delim(file, header = F, sep = "|", stringsAsFactors = F)), error = function(e) {return(NULL)})})
      names(tablesData) <- basename(files)
      tablesData <- tablesData[!sapply(tablesData, is.null)]
    }
  
  # below is just copy pasted from 'FedData::get_ssurgo_study_area' (v2.5.7)
    SSURGOTableMapping <- tablesData[["mstab.txt"]][, c(1,5)]
    names(SSURGOTableMapping) <- c("TABLE", "FILE")
    SSURGOTableMapping[, "FILE"] <- paste(SSURGOTableMapping[,"FILE"], ".txt", sep = "")
    tablesData <- tablesData[as.character(SSURGOTableMapping[,"FILE"])]
    tablesHeads <- tablesHeaders[as.character(SSURGOTableMapping[,"TABLE"])]
    notNull <- (!sapply(tablesData, is.null) & !sapply(tablesHeads,is.null))
    tablesData <- tablesData[notNull]
    tablesHeads <- tablesHeads[notNull]
    tables <- mapply(tablesData, tablesHeads, FUN = function(theData,theHeader) {
      names(theData) <- names(theHeader)
      return(theData)
    })
    names(tables) <- names(tablesHeads)
    tables <- extract_ssurgo_data(tables = tables, mapunits = as.character(unique(mapunits$MUKEY)))
    
    # save results as ESRI shapefile and csv (adapted from FedData::get_ssurgo v2.5.7)
    suppressWarnings(rgdal::writeOGR(as(mapunits, 'Spatial'), dsn = normalizePath(paste0(extraction.dir,"/.")), layer = paste0(label, "_SSURGO_Mapunits"), driver = "ESRI Shapefile", overwrite_layer = TRUE))
    junk <- lapply(names(tables), function(tab) {readr::write_csv(tables[[tab]], path = paste(extraction.dir, "/", label, "_SSURGO_", tab, ".csv", sep = "")) })

    return(list(spatial = mapunits, tabular = tables))
}
```

This function parses a SSURGO/STATSGO2 style list of tables to extract
the soil data needed for SWAT+. These data are reshaped into a single
table `usersoil` (the return value) that maps mukeys to the relevant
information on the dominant soil component (ie the component with
maximal comppct\_r). The output table is formatted in the style expected
for the CSV file “usersoil.csv” in SWAT+ AW, with an additional column,
`pmiss` indicating for each mukey the percent of soil variables which
are missing (NA).

The code for this function is adapted from the missing data filler
script in [this
post](https://hydrologicou.wordpress.com/r-script-to-generate-missing-records-in-acrswat-ssurgo-database/),
which in turn is based on snippets from [this
example](https://r-forge.r-project.org/scm/viewvc.php/*checkout*/docs/soilDB/gSSURGO-SDA.html?root=aqp).

``` r
my_usersoil = function(soils.tab, my.mukeys=NA)
{
  # ARGUMENTS:
  #
  # `soils.tab` a named list of STATSGO2/SSURGO dataframes (containing `component`, `chorizon`, and `chtexturegrp`)
  # `my.mukeys` an (optional) vector of integer-valued mapunit keys to process and include in the output
  #
  # RETURN VALUE:
  #
  # a data.frame in the format of "usersoil.csv" (with columns `MUID`, `SEQN`, `SNAM`, etc)
  
  ## define some magic numbers
   
  # number of horizons to write for each soil component
  n.hz = 10
  
  # assume albedo is reduced to 60% for moist soil vs dry soil (as it is reported in SSURGO/STATSGO2)
  albedo.const = 0.6
  
  # van Bemmelen factor for converting organic matter weight to organic carbon content
  cbn.const = (1/0.58)
  
  # suppress overly verbose friendly warnings about grouping variables from dplyr
  options(dplyr.summarise.inform=FALSE) 
  
  ## prepare the source data tables

  # check for incomplete/missing `mukey` argument
  if(any(is.na(my.mukeys)))
  {
    print('NA(s) detected in `my.mukeys` argument, processing all mukeys in input dataset...')
    my.mukeys = sort(soils.tab$component$mukey)
  }
  
  # eliminate any duplicate mukey entries and count number of unique keys
  my.mukeys = unique(my.mukeys)
  n.mukeys = length(my.mukeys)
  
  # prepare components table by assigning (to each mukey) the soil component with max coverage 
  my.comp = data.frame(mukey=my.mukeys) %>%
    left_join(soils.tab$component, by='mukey') %>%  
    group_by(mukey) %>% 
    slice(which.max(comppct.r)) %>% 
    arrange(cokey) %>%
    as.data.frame()
  
  # prepare soil layer (horizon) data for the dominant components. Most components have multiple horizons
  my.chrz = my.comp %>%
    left_join(soils.tab$chorizon, by='cokey') %>%
    group_by(cokey)
  
  # prepare soil texture group data 
  my.chtx = soils.tab$chtexturegrp %>%
    filter(chkey %in% my.chrz$chkey) %>%
    arrange(chkey)

  ## prepare the left half of the table
  # all unnumbered parameters (ie those not specific to a single horizon) are stored here
  my.df.left = data.frame(
    
    # map unit key (mukey)
    MUID = my.comp$mukey,
    
    # soil sequence (component) key (cokey) for the dominant component
    SEQN = my.comp$cokey,
    
    # soil name (character code that serves as key to soils.csv lookup table for raster integer codes)
    SNAM = rep(NA, n.mukeys),
    
    # soil interpretation record (SIR, Soils-5, SOI-5, or S-5, a decommissioned/archived database)
    S5ID = rep(NA, n.mukeys),
    
    # percentage of the mapunit occupied by this component
    CMPPCT = my.comp$comppct.r,
    
    # number of soil layers in this component
    NLAYERS = my.chrz %>% 
      dplyr::summarize(n = n()) %>%
      pull(n), 
    
    # USDA Soil Survey hydrological group (A, B, C, D) 
    HYDGRP = my.comp$hydgrp,
    
    # max rooting depth of soil profile (in mm = 10*cm)
    SOL_ZMX = my.chrz %>% 
      dplyr::summarize(depth = 10*max(hzdepb.r)) %>% 
      pull(depth),
    
    # unavailable in SSURGO
    ANION_EXCL = rep(NA, nrow(my.comp)),
    
    # unavailable in SSURGO
    SOL_CRK = rep(NA, nrow(my.comp)),
    
    # texture description (not processed by the model)
    TEXTURE = my.chrz %>%
      dplyr::summarize(texture = my.chtx$texture[match(chkey, my.chtx$chkey)]) %>%
      dplyr::summarize(texture_code = paste0(texture, collapse='-')) %>%
      as.data.frame() %>%
      pull(texture_code)
    
  )
  
  ##  compile (as list) all parameters specific to a single horizon, later reshaped to form right half of table
  # lapply call iterates over horizon numbers, building a table (spanning all mukeys) for each one
  my.list.right = lapply(1:n.hz, function(hz.idx) my.chrz  %>% dplyr::summarize(
    
    # depth from soil surface to bottom of layer (in mm = 10*cm)
    SOL_Z = 10*hzdepb.r[hz.idx],
    
    # moist bulk density (Mg/m3)
    SOL_BD = dbovendry.r[hz.idx],
    
    # available water capacity (H20/soil)
    SOL_AWC = awc.r[hz.idx],
    
    # saturated hydraulic conductivity (in mm/hr = 3600 s/hr / 1000 micrometers/mm)
    SOL_K = 3.6*ksat.r[hz.idx],
    
    # organic carbon content (% of soil weight) estimated from organic matter via van Bemmelen factor
    SOL_CBN = cbn.const*om.r[hz.idx],
    
    # clay content (% of soil weight)
    CLAY = claytotal.r[hz.idx],
    
    # silt content (% of soil weight)
    SILT = silttotal.r[hz.idx],
    
    # sand content (% of soil weight)
    SAND = sandtotal.r[hz.idx],
    
    # rock fragment content (% of total weight)
    ROCK = 100 - sieveno10.r[hz.idx],
    
    # USLE equation soil erodibility (K) factor 
    USLE_K = kwfact[hz.idx],
    
    # electrical conductivity (dS/m)
    SOL_EC = ec.r[hz.idx],
    
    # soil CaCo3 (% of soil weight)
    CAL = caco3.r[hz.idx],
    
    # soil pH
    PH = ph1to1h2o.r[hz.idx]
    
  ) %>% 
    
    # for this next parameter, we re-use the single SSURGO value on all horizons
    # moist soil albedo (ratio of reflected/incident)
    mutate(SOL_ALB = albedo.const*my.comp$albedodry.r) %>% 
    
    # omit the cokeys (they are included as SEQN in my.df.left, above)
    select(-cokey) %>% 
    
    # and, finally, you've reached the end of this really long lapply call
    as.data.frame)
  
  # append horizon number to each column name in preparation for list -> table reshape
  for(idx.hz in 1:n.hz) { names(my.list.right[[idx.hz]]) = paste0(names(my.list.right[[idx.hz]]), idx.hz) }
  
  # reshape into a single table
  my.df.right = do.call(cbind, my.list.right)
  
  # bind the left and right sides of the table and reorder to match input `my.mukeys`
  usersoil.df = cbind(my.df.left, my.df.right)[match(my.mukeys, my.df.left$MUID),]
  
  # number of variables expected in the left table (omit SNAM, S5ID, ANION_EXCL, SOL_CRK)
  nvars.left = ncol(my.df.left) - 4
  
  # number of variables expected in the right table (depends on the number of horizons)
  nvars.right = usersoil.df$NLAYERS * ncol(my.list.right[[1]])
  nvars.all = nvars.left + nvars.right

  # add a column indicating the percent of expected variables which were NA
  usersoil.df$pmiss = sapply(1:n.mukeys, function(idx.r) sum(is.na(usersoil.df[idx.r, 1:nvars.all[idx.r]]))/nvars.all[idx.r])

  # add OBJECTID column and return the data frame
  return(cbind(data.frame(OBJECTID=1:n.mukeys), usersoil.df))
}
```

This function returns a (case-insensitive) expression-matching lookup
table for matching NVC plant community classifications to SWAT+ plant
codes. The former are documented
[here](http://usnvc.org/data-standard/natural-vegetation-classification),
and the latter can be found in the `plants_plt` table of the SWAT+
‘datasets’ SQLite file.

``` r
my_plants_plt = function(nvc.df)
{
  # ARGUMENTS:
  #
  # `nvc.df`: a dataframe with columns `NVC_DIV`, `NVC_MACRO`, and `NVC_GROUP`
  #
  # RETURN VALUE:
  #
  # the `nvc.df` dataframe with two appended columns, `swatcode` and `swatdesc`, containing
  # the matching SWAT+ plant code and its description 
  # 
  # DETAILS:
  #
  # The lookup table is specified below, in a list of 1-row dataframes, each containing the fields `swatdesc`
  # and `swatcode`. These strings should match the `description` and `name` fields in a row of `plants_plt`.
  # The other 3 fields -- `kwdiv`, `kwmacro`, and `kwgroup` -- are keywords to filter the NCV divisions, macros,
  # and groups (a nested hierarchical classification), where the pipe operator `|` functions as a logical OR.
  #
  # Note that order matters in this list, as earlier entries are superceded by later ones whenever
  # there is any overlap in the subset of NVC classifications specified by the `kw*` arguments.
  
  plt.list = list(
    
    # generic for sparsely vegetated or barren land
    data.frame(swatdesc='barren',
               swatcode='barr',
               kwdiv='barren|urban|mines'),
    
    # generic for sparsely vegetated land
    data.frame(swatdesc='barren_or_sparsley_vegetated',
               swatcode='bsvg',
               kwgroup='scree|badland'),
    
    # generic for cropland
    data.frame(swatdesc='agricultural_land_generic', 
               swatcode='agrl',
               kwdiv='agricultural'),
    
    # generic for hay and pasture (see Appendix A) and introduced grassland/forbland
    data.frame(swatdesc ='tall_fescue', 
               swatcode='fesc',
               kwdiv='hay|introduced'),
    
    # generic for shrublands (notice one instance is misspelled!)
    data.frame(swatdesc ='range_brush_temperate_mountain_systems', 
               swatcode='rngb_tems', 
               kwgroup='shrubland|shubland|scrub'),
    
    # generic for warmer/drier shrublands 
    data.frame(swatdesc ='range_brush_temperate_steppe', 
               swatcode='rngb_test', 
               kwgroup='deciduous shrubland|sagebrush scrub'),
    
    # generic for shrublands (notice one instance is misspelled!)
    data.frame(swatdesc ='mixed_grassland/shrubland', 
               swatcode='migs', 
               kwgroup='sagebrush stepp'),
    
    # generic for grasslands
    data.frame(swatdesc ='range_grasses_temperate_mountain_systems', 
               swatcode='rnge_tems', 
               kwgroup='prairie|grassland'),
    
    # generic for semi-arid grasslands
    data.frame(swatdesc ='range_grasses_temperate_steppe', 
               swatcode='rnge_test', 
               kwgroup='semi-desert grassland'),
    
    # generic for pine-dominated forests, which I assign to recently-disturbed areas (harvest/fire) 
    data.frame(swatdesc='pine',  
               swatcode='pine',
               kwdiv='forest|woodland|disturbed', 
               kwgroup='pine|disturbed'),
    
    # generic mixed-wood class
    data.frame(swatdesc='forest_mixed_temperate_mountain_systems', 
               swatcode='frst_tems', 
               kwdiv='forest|woodland',
               kwgroup='mountain-mahogany|swamp|riparian'),
    
    # generic for evergreen temperate steppe forests
    data.frame(swatdesc='forest_evergreen_temperate_steppe', 
               swatcode='frse_test', 
               kwdiv='forest|woodland', 
               kwgroup='plains|open woodland'),
    
    # I interpret `poplar` as a generic Populus class
    data.frame(swatdesc='poplar',  
               swatcode='popl',
               kwdiv='forest|woodland', 
               kwgroup='aspen|cottonwood'),
    
    # generic for non-pine evergreen temperate mountainous forests
    data.frame(swatdesc='forest_evergreen_temperate_mountain_systems', 
               swatcode='frse_tems', 
               kwdiv='forest|woodland', 
               kwgroup='fir|juniper'),
    
    # a specific class for lodgepole pine forests
    data.frame(swatdesc='lodge_pole_pine', 
               swatcode='ldgp', 
               kwdiv='forest|woodland',
               kwgroup='lodgepole'),
    
    # a more specific class for white-spruce forests
    data.frame(swatdesc='white_spruce',  
               swatcode='wspr',
               kwdiv='forest|woodland', 
               kwgroup='dry-mesic spruce'),
    
    # generic for forested wetlands
    data.frame(swatdesc ='wetlands_forested',  
               swatcode='wetf',
               kwgroup='swamp forest'),
    
    # generic for wet shrublands
    data.frame(swatdesc ='herbaceous_wetland', 
               swatcode='wehb', 
               kwgroup='fen|marsh|wet meadow|vernal pool'),
    
    # herbaceous alpine tundra 
    data.frame(swatdesc='herbaceous_tundra',  
               swatcode='tuhb',
               kwdiv='tundra'),
    
    # barren alpine tundra 
    data.frame(swatdesc='bare_ground_tundra', 
               swatcode='tubg', 
               kwdiv='tundra', 
               kwgroup='scree')
    
  ) 

  # combine rows from all dataframes, filling missing kw fields with empty character ('') 
  plt = plt.list %>% bind_rows %>% mutate_all(~replace(., is.na(.), ''))
  
  # copy the input dataframe, appending two new (empty) columns
  nvc.out.df = nvc.df %>% mutate(swatcode=NA, swatdesc=NA)

  # fill in SWAT+ codes: note that later entries of `plt` overwrite earlier ones 
  for(idx.plt in 1:nrow(plt))
  {
    # grep for keywords in the NCV classifications
    idx.div = grepl(plt[idx.plt, 'kwdiv'], nvc.df$NVC_DIV, ignore.case=TRUE)
    idx.group = grepl(plt[idx.plt, 'kwgroup'], nvc.df$NVC_GROUP, ignore.case=TRUE)
    
    # write SWAT+ code to each matching entry 
    nvc.out.df[idx.div & idx.group, 'swatcode'] = plt[idx.plt, 'swatcode']
    nvc.out.df[idx.div & idx.group, 'swatdesc'] = plt[idx.plt, 'swatdesc']
  }
  
  return(nvc.out.df)
}
```

It took a bit of work to construct the function above: The following may
be helpful for improving the lookup table, or when extending it to new
areas, and/or new releases of SWAT+ and the NCV.

To begin cross-referencing SWAT+ plant codes with the NVC
classifications, we start with packages
[`DBI`](https://cran.r-project.org/web/packages/DBI/vignettes/DBI-1.html)
and [`RSQLite`](https://github.com/r-dbi/RSQLite) to interface with
SWAT+ SQLite databases For example, to load the `plants.plt` in
“swatplus\_datasets.sqlite”, do:

``` r
# library(DBI) 
# library(RSQLite)
# swat.ds.conn = dbConnect(SQLite(), 'H:/UYRW_SWAT/SWATPlus/Databases/swatplus_datasets.sqlite')
# plants_plt = dbReadTable(swat.ds.conn, 'plants_plt')
# dbDisconnect(swat.ds.conn)
```

Some (now incomplete) reference information on the `plants_plt` data can
be found in [Appendix A of the SWAT IO file docs
(2012)](https://swat.tamu.edu/media/69419/Appendix-A.pdf). Since SWAT+
is fairly new, and under active development, I would expect this
document to be updated in the near future. For now, some guesswork was
need to match the `description` field to each [NVC group
category](http://usnvc.org/explore-classification/) in the UYRW.

[`data.tree`](https://cran.r-project.org/web/packages/data.tree/vignettes/data.tree.html)
can be very helpful for parsing this kind of data, with clean printouts
of nested lists of strings.

In addition to having different file and data structures, SWAT2012 and
SWAT+ (currently) differ in the default parameter sets that are included
in each release. For example, while most of the plant growth parameters
appearing SWAT2012’s reference database ‘crop’ table can be found in the
SWAT+ database, there are many plant codes in SWAT+ that have not yet
been added to SWAT2012.

This helper function translates the relevant data from the SWAT+
reference database into a form that can be written to the SWAT2012
reference database (mdb), so the missing plant codes can be used in a
SWAT2012 model

``` r
my_plants2swat = function(crop_in, swatplus_db)
{
  # ARGUMENTS:
  #
  # `crop_in`: 'crop' table (as data frame) from the SWAT2012's MS Access (mdb) reference database
  # `swatplus_db`: path to the mySQL (sql) reference database 
  #
  # RETURN VALUE:
  #
  # A named list with two entries;
  #
  # `crop_out`: data frame in the same form as `crop_in`, appended with any data on plant codes
  # found in `swatplus_db` but missing from `crop_in`. 
  #
  # `lookup`: a data frame matching the plant codes assigned in `crop_out` to their original
  # values in the SWAT+ database. These plant codes are modified only when it is necessary to
  # satisfy SWAT2012's 4-letter naming requirement. Auto-generated names are all of the form 'ZZ**'
  #
  # BEHAVIOUR: 
  #
  # Note that two fields in the 'crop' table, 'FERTFIELD' and 'OpSchedule', are assigned the default
  # values of 0 and 'RNGB' in all appended rows, as we could not find suitable surrogates in
  # `swatplus_db`. These fields appear to be missing from the SWAT theory and I/O documentation. As
  # far as we can tell, they are unimportant to our implementation of SWAT/SWAT+
  
  # open the SWAT+ database to pull a copy of four tables containing land use parameters
  swatplus.con = RSQLite::dbConnect(RSQLite::SQLite(), swatplus_db)
  
  # ... the main data table...
  plants.plt = dbReadTable(swatplus.con, 'plants_plt')
  
  # ... and linked tables for Manning's 'n' for overland flow, runoff curve numbers
  landuse.lum = dbReadTable(swatplus.con, 'landuse_lum')
  ovn.lum = dbReadTable(swatplus.con, 'ovn_table_lum')
  cn.lum = dbReadTable(swatplus.con, 'cntable_lum')
  
  # close the connection
  RSQLite::dbDisconnect(swatplus.con)
  
  # The SWAT+ database table 'plants_plt' (copied into R as `plants.plt`) contains most of the
  # parameters needed in the 'crop' table in the SWAT2012 database. The field names are different,
  # however, so these need to be translated. A handful of parameters also need to be fetched from
  # other tables. These are linked to 'plants_plt' via 'landuse_lum'
  
  # identify plant codes common to both databases
  codes.common = crop_in$CPNM[crop_in$CPNM %in% toupper(plants.plt$name)]
  
  # identify plant codes in 'plants_plt' which are missing from SWAT2012 database
  codes.toadd = plants.plt$name[!(toupper(plants.plt$name) %in% codes.common)]
  
  # fetch variables located outside `plants.plt`, append as new columns
  ovn.vn = 'ovn_mean'
  cn.vn = c('cn_a', 'cn_b', 'cn_c', 'cn_d')
  idx.lum = match(plants.plt$name, sapply(landuse.lum$name, function(nm) strsplit(nm, '_lum')[[1]]))
  plants.plt[, ovn.vn] = ovn.lum[landuse.lum$ov_mann_id[idx.lum], ovn.vn]
  plants.plt[, cn.vn] = cn.lum[landuse.lum$cn2_id[idx.lum], cn.vn]
  
  # identify 'trees' and translate 'plnt_typ' to integer code (see 'swat2009-theory.pdf', p.311)
  plants.plt$plnt_typ[plants.plt$bm_tree_acc>0 & plants.plt$bm_tree_max>0] = 'trees'
  idc.codes = c(NA, NA, NA, 'warm_annual', 'cold_annual', 'perennial', 'trees')
  plants.plt$plnt_typ = match(plants.plt$plnt_typ, idc.codes)
  
  # add dummy data to two missing fields
  plants.plt$FERTFIELD = rep(0, nrow(plants.plt))
  plants.plt$OpSchedule = rep('RNGB', nrow(plants.plt))
  
  # All of the necessary data has now been added to `plants.plt`. Next we trim this table to include
  # only plant codes which are missing from `crop_in` and modify field names and row keys to match
  # with the syntax of SWAT2012's 'crop'.
  
  # create a translation table (SWAT+/plants_plt -> SWAT2012/crop)
  swat2012.lu = c(CPNM='name',
                  IDC='plnt_typ',
                  CROPNAME='description',
                  BIO_E='bm_e',
                  HVSTI='harv_idx',
                  BLAI='lai_pot',
                  FRGRW1='frac_hu1', 
                  LAIMX1='lai_max1', 
                  FRGRW2='frac_hu2', 
                  LAIMX2='lai_max2', 
                  DLAI='hu_lai_decl', 
                  CHTMX='can_ht_max', 
                  RDMX='rt_dp_max', 
                  T_OPT='tmp_opt', 
                  T_BASE='tmp_base', 
                  CNYLD='frac_n_yld',
                  CPYLD='frac_p_yld',
                  BN1='frac_n_em',
                  BN2='frac_n_50',
                  BN3='frac_n_mat',
                  BP1='frac_p_em',
                  BP2='frac_p_50',
                  BP3='frac_p_mat',
                  WSYF='harv_idx_ws',
                  USLE_C='usle_c_min',
                  GSI='stcon_max',
                  VPDFR='vpd',
                  FRGMAX='frac_stcon',
                  WAVP='ru_vpd',
                  CO2HI='co2_hi',
                  BIOEHI='bm_e_hi',
                  RSDCO_PL='plnt_decomp',
                  OV_N='ovn_mean',
                  CN2A='cn_a',
                  CN2B='cn_b',  
                  CN2C='cn_c',  
                  CN2D='cn_d', 
                  FERTFIELD='FERTFIELD',
                  ALAI_MIN='lai_min',
                  BIO_LEAF='bm_tree_acc',
                  MAT_YRS='yrs_mat',
                  BMX_TREES='bm_tree_max',
                  EXT_COEF='ext_co',
                  BM_DIEOFF='bm_dieoff',
                  OpSchedule='OpSchedule')
  
  # translate column names of `plants.plt` (SWAT+) to those of `crop` (SWAT2012)
  plants.tocrop = plants.plt[plants.plt$name %in% codes.toadd, swat2012.lu]
  colnames(plants.tocrop) = names(swat2012.lu)
  
  # append new row key columns
  keys.tocrop = (1:length(codes.toadd)) + nrow(crop_in) + 1
  plants.tocrop = cbind(data.frame(OBJECTID=keys.tocrop, ICNUM=keys.tocrop), plants.tocrop)
  
  # generate new 4-letter codes (of form 'ZZ**') to replace longer SWAT+ plant codes
  idx.namechange = nchar(plants.plt$name) > 4 
  seq.namechange = (1:sum(idx.namechange)) - 1
  suffix.namechange = cbind(LETTERS[(seq.namechange %/% 26) + 1], LETTERS[(seq.namechange %% 26) + 1])
  codes.namechange = paste0('ZZ', apply(suffix.namechange, 1, paste, collapse=''))
  
  # lookup vector that switches all codes to uppercase, and subs in 4-letter versions as needed
  codes.new = setNames(toupper(plants.plt$name), nm=plants.plt$name)
  codes.new[idx.namechange] = codes.namechange
  plants.tocrop$CPNM = codes.new[match(plants.tocrop$CPNM, names(codes.new))]
  
  # return output as list
  return(list(crop_out=plants.tocrop, lookup=codes.new))
  
}
```

Convenience functions below are for handling I/O text files associated
with SWAT2012 simulations, and for managing parameters and system calls
to the SWAT executables reads daily simulation output text files from
SWAT+

``` r
my_read_output = function(out.path, varname=character(0), set.units=TRUE, add.dates=TRUE)
{
  # ARGUMENTS:
  #
  # `out.path`, character string, full path to one of the SWAT+ output (.txt) files
  # `varname`, character string vector, names of the variables to load (or 'all')
  # `set.units`, boolean indicating whether to assign units to output columns
  # `add.dates`, boolean indicating whether to append a Date object column
  #
  # RETURN VALUE:
  #
  # If `varname` is unspecified, returns a list containing the time period and variable
  # names available, as well as the vector of unique geometry ID codes (`set.units`, and
  # `add.dates` are ignored in this case).
  #
  # If `varname` is a string, or vector of strings, returns a data frame containing the 
  # numerical data from any and all matching variable names. 
  #
  # DETAILS:
  # 
  # Any unmatched string(s) in `varname` are ignored, with the exception of `varname='all'`,
  # which prompts the function to return all available variables.
  #
  # Three indexing variables are included in every data frame: 'jday' (Julian day in year),
  # 'yr' (year), and 'gis_id' (ID code for this feature in QSWAT+ shapefiles). For non-
  # indexing variables, the columns of the data frames are of type Unit, with units assigned
  # based on the headers data of the output text file.
  #
  # `add.dates==TRUE` consolidates the 'jday', 'mon', 'day', 'yr' columns into a single 
  # vector of Date objects (ie they are omitted from the data frame). 
  #
  
  # line numbers of the headers for variable names and units
  ln.header = c(varname=2, unit=3)
  
  # scan the headers line to get all listed output variable names
  out.varname = scan(out.path, what=character(), skip=ln.header['varname']-1, nlines=1)
  out.unit = scan(out.path, what=character(), skip=ln.header['unit']-1, nlines=1)
  n.varname = length(out.varname)
  
  # the first few columns are keys for location/time - their names appear on the units line
  n.key = length(out.unit) - n.varname
  is.key = 1:length(out.unit) %in% 1:n.key
  key.varname = out.unit[is.key]
  out.unit = out.unit[!is.key]
  
  # replace unusual units with strings that R can understand and name them to match variables
  out.unit[out.unit == 'ha-m'] = 'ha m'
  out.unit[out.unit %in% c('kgN', 'kgP')] = 'kg'
  names(out.unit) = out.varname
  
  # build a list of column types for keys, then for the rest of the data matrix
  key.class = c(rep('integer', n.key-1), 'character')
  out.class = rep('numeric', n.varname)
  
  # complete set of column classes and names
  load.what = c(key.class, out.class)
  all.varname = c(key.varname, out.varname)
  
  # three of the key columns will be imported even when they aren't specified in `varname`
  key.include = c('jday', 'yr', 'gis_id')
  
  # two special `varname` assignments prompt the function to parse all available variables 
  if(all(varname == 'all') | length(varname)==0)
  {
    # default behaviour with `varname` unassigned is to scan for info about a simulation
    if(length(varname)==0)
    {
      # retrieve important key columns only (ignore numerical data)
      varname = key.include
      return.data = FALSE
      
    } else {
      
      # this mode reads the entire file returning all data
      varname = all.varname
      return.data = TRUE
      
    }
    
  } else {
    
    # important keys are appended to any supplied variable name arguments
    varname = unique(c(key.include, varname))
    return.data = TRUE
    
  }
  
  # load the requested columns and efficiently skip the others using `fread`
  load.idx = all.varname %in% varname
  load.varname = all.varname[load.idx]
  dat.out = as.data.frame(setNames(fread(out.path, 
                                         header=FALSE, 
                                         skip=max(ln.header),  
                                         colClasses=load.what, 
                                         drop=which(!load.idx)), load.varname))
  
  # collect info from key columns
  n.obs = nrow(dat.out)
  gis.ids = unique(dat.out$gis_id)
  start.date = as.Date(paste(dat.out$yr[1], dat.out$jday[1], sep='-'), format='%Y-%j')
  end.date = as.Date(paste(dat.out$yr[n.obs], dat.out$jday[n.obs], sep='-'), format='%Y-%j')
  dates = c(first=start.date, last=end.date)
  n = c(n_obs=n.obs, n_days=n.obs/length(gis.ids))
  
  # return this information in default mode
  if(!return.data)
  {
    # return the info in a list
    return(list(n=n, dates=dates, name=all.varname, gis_ids=gis.ids))
    
  }
  
  # pull column names before making final changes
  dat.names = names(dat.out)
  
  # units are assigned to the state variables if requested
  if(set.units)
  {
    # loop over column names matching non-key (ie state variable) values
    for(idx.col in which(!(dat.names %in% key.varname)))
    {
      # units were read from headers line
      unit.col = out.unit[names(dat.out)[idx.col]]
      dat.out[,idx.col] = units::set_units(dat.out[,idx.col], unit.col, mode='standard')
    }
  }
  
  # assign dates if requested 
  include.names = dat.names
  if(add.dates)
  {
    # define the sequence of dates appearing in the dataset
    ts.dates = seq(start.date, end.date, 'day')
    
    # this should be equivalent to `as.Date(paste(yr, mon, day, sep='-')`, but faster
    all.dates = rep(ts.dates, each=n['n_obs']/n['n_days'])
    
    # append to `dat.out` as new column
    dat.out = setNames(cbind(all.dates, dat.out), c('date', dat.names))
    
    # omit redundant columns
    include.names = names(dat.out)[!(names(dat.out) %in% c('jday', 'mon', 'day', 'yr'))]
  }
  
  return(dat.out[, include.names])
}
```

reads simulation output text files from SWAT2012: output.rch,
output.sub, output.hru

``` r
my_swat_rout = function(output.path, varname=character(0), origin.date=character(0))
{
  # ARGUMENTS:
  #
  # `output.path`, character string, path to either 'output.rch', 'output.sub', or 'output.hru'
  # `varname`, character string vector of variable names to load from the output file
  # `origindate`, (optional) 'Date' class object specifying first day of simulation period
  #
  # RETURN VALUE:
  #
  # A data frame containing the data from the files at `output.path` (with comment lines 1-9
  # omitted). By default returns the entire dataset, but a subset of columns can be specified in
  # `varname`.
  #
  # If `origindate` is supplied, the function appends a column containing the dates for
  # each data row (note that this assumes a daily time step). It also returns the 'RCH', 'MON'
  # columns (if they are not already specified in varname)
  
  # line number of the headers
  out.idx = 9
  
  # identify the output text file type
  out.ext = gsub('.', '', extension(output.path), fixed=TRUE)
  
  # define key fields that vary by output type
  key.names = list(rch=c('RCH', 'GIS', 'MON'), 
                   sub=c('SUB'), 
                   hru=c('LULC', 'HRU', 'GIS', 'SUB', 'MGT'))[[out.ext]]
  
  # scan the headers line and note that R can't parse the delimiters in use here
  out.header = paste0(scan(output.path, what=character(), skip=out.idx-1, nlines=1), collapse=' ')
  
  # bugfix for 'LAT Q' variable in .sub files: switch to 'LAT_Q' 
  out.header = gsub('LAT Q(mm)LATNO3', 'LAT_Q ', out.header, fixed=TRUE)
  
  # use regexp to identify whitespace and units delimiting individual variables
  rch.regexp = '(km2)|(cms)|(tons)|(mg/L)|(kg)|(mg)|(ct)|(TOT)|(Mg/l)|(degc)'
  sub.regexp = '(mm)|(kg/ha)|(kg/h)|(t/ha)|(mic/L)'
  hru.regexp = '(dgC)|(MJ/m2)'
  out.regexp = paste0(c('(\\s)', rch.regexp, sub.regexp, hru.regexp), collapse='|')
  out.header.parsed = strsplit(out.header, out.regexp)[[1]]
  out.header.parsed = out.header.parsed[nchar(out.header.parsed) > 0]
  
  # bugfix: field 'MON' is listed in header of hru and sub files, but not in numeric table
  if(out.ext != 'rch') { out.header.parsed = out.header.parsed[out.header.parsed!='MON'] }
    
  # bugfix: first column is unnamed in sub and rch files, but it appears to be a dummy variable
  if(out.ext != 'hru') { out.header.parsed = c('DUMMY', out.header.parsed) }
  
  # default behaviour is to load all fields except 'DUMMY'
  if(length(varname)==0)
  {
    varname = out.header.parsed[out.header.parsed != 'DUMMY']
    
  } else {
    
    # the integer fields should always be returned
    varname = unique(c(key.names, varname))
    
  }
  
  # count the number of requested fields
  n.col = length(out.header.parsed) 
  
  # build a list of column types - most are doubles
  fread.what = rep('numeric', n.col)
  fread.what[out.header.parsed == 'DUMMY'] = 'character'
  fread.what[out.header.parsed %in% key.names] = 'integer'
  fread.what[out.header.parsed == 'LULC'] = 'character'
  
  # verify that the requested variable name was printed to this output file
  idx.varname.found = varname %in% out.header.parsed
  if(!all(idx.varname.found))
  {
    err.msg1 = paste('The following variable name(s) not found in', basename(output.path), ':')
    err.msg2 = paste(varname[!idx.varname.found], collapse=', ')
    stop(paste(err.msg1, err.msg2, collapse='\n'))
  }
  
  # reorder varname to match ordering in headers
  varname = varname[order(match(varname, out.header.parsed))]
  
  # index the columns to keep and drop
  idx.read = out.header.parsed %in% varname
  idx.drop = which(!idx.read)
  
  # load the requested columns using fread
  dat.out = fread(output.path, 
                  header=FALSE, 
                  skip=out.idx, 
                  col.names=varname, 
                  colClasses=fread.what, 
                  drop=idx.drop)
  
  # warn of any coercion errors in the numeric data and replace invalid entries with NA
  idx.col.invalid = sapply(dat.out, class) != fread.what[idx.read]
  if(any(idx.col.invalid))
  {
    # identify the variables and print the warning
    names.col.invalid = names(dat.out)[idx.col.invalid]
    print('warning: type coercion failed in the following variable(s)')
    print(names.col.invalid)
    print('coercing to numeric...')
    
    # perform the coercion
    invisible(sapply(names.col.invalid, function(vn) {
      set(dat.out, j=varname, value=as.numeric(dat.out[[vn]]))
    }))
    
  }
  
  # if dates requested, append the row, then finish
  if(length(origin.date)>0)
  {
    n.id = length(unique(dat.out[[toupper(out.ext)]]))
    n.days = nrow(dat.out)/n.id
    dates.out = rep(seq.Date(as.Date(origin.date), by='day', length.out=n.days), each=n.id)
    return(cbind(date=dates.out, dat.out))
    
  } else {
    
    return(dat.out)
    
  }
  
}
```

system call to run a SWAT simulation

``` r
my_call_swat = function(textio.dir, exec_path)
{
  # ARGUMENTS:
  #
  # `textio.dir`, character string path to the text input file directory of a SWAT2012 model
  # `exec_path`, character string path to swat executable (.exe, likely in SWATEditor directory)
  #
  
  # shell command prefix to change to the directory (avoids changing R's working directory)
  shell.prefix = paste0('pushd ', normalizePath(textio.dir), ' &&')
  
  # build system call and run SWAT
  syscall.string = paste(shell.prefix, tools::file_path_sans_ext(normalizePath(exec_path)))
  shell(syscall.string)
}
```

parse SWAT text file lines to return: value, name, comment fields for
non-vector parameters

``` r
my_swat_readpar = function(linetext)
{
  # ARGUMENTS:
  #
  # `linetext`, character string vector, the (line by line) raw text from a SWAT config file 
  #
  # RETURN VALUE:
  #
  # dataframe with nrows equal to the number of lines in `linetext` that match a standard
  # non-vector variable assignment pattern (see DETAILS). The five named columns are:
  #
  # line = (integer) line number, indicates entry of `linetext` that was parsed
  # name = (character) the parameter name (usually UPPERCASE)
  # value = (character) the parameter value
  # comment = (character) the comment string (if any)
  # type = (character) either 'character', 'integer', 'numeric', or 'unknown'
  #
  # DETAILS:
  #
  # Most SWAT parameters appear as lines of the form "<value>|<name>:<comment>", where <value>
  # may be of type character, integer, or real. This function parses a line of text in this
  # syntax, separating the three components, trimming whitespace, and returning the result as
  # character.
  #
  # Type detection is based on the presence/absence of non-numeric characters and the decimal
  # symbol ('.'). According to the SWAT docs, the distinction between integers are reals is
  # important - eg. there could be problems associated with writing the integer 3 as a float
  # such as "3.00" (and vice versa).
  # 
  # When there is no template value assigned in the input `linetext` to pattern-match, the 
  # function looks for a comment that ends with 'file' (indicating 'character' type), and
  # failing that, it assigns 'unknown' type.
  # 
  
  # regexp to find standard pattern
  idx.match = grepl('(.+)(\\|)(.+)(:)(.+)', linetext)
  linetext.m = linetext[idx.match]
  
  # split into <value><name><comment> (literal '0-1' is bugfix for missing ':' in 'BFLO_DIST')
  out.ms = strsplit(linetext.m, '(\\s*\\|\\s*)|(\\s*:\\s*)|(\\s0-1\\s)', perl=TRUE)
  
  # trim leading and trailing whitepace
  out.mst = lapply(out.ms, function(vec) gsub('($\\s*)|(^\\s*)', '', vec, perl=TRUE))
  
  # reshape into matrix, fuse comments that got split because they have a ':' symbol
  out.list = lapply(out.mst, function(vec) c(vec[2:1], paste(vec[-(1:2)], collapse=': ')))
  
  # initialize output dataframe
  df.out = cbind(which(idx.match), data.frame(do.call(rbind, out.list)))
  names(df.out) = c('line', 'name', 'value', 'comment')
  
  # initialize output type column with 'integer'
  df.out$type = 'integer'
  
  # determine character and numeric value types
  df.out$type[grepl('[0-9,A-z]\\.[A-z]', df.out$value, perl=TRUE)] = 'character'
  df.out$type[grepl('\\d+\\.\\d+', df.out$value, perl=TRUE)] = 'numeric'
  
  # deal with unknowns
  idx.unknown = df.out$value == ''
  df.out$type[idx.unknown] = 'unknown'
  df.out$type[ endsWith(df.out$comment, 'file') & idx.unknown ] = 'character'
  
  # finish
  return(df.out)
}
```

write parameter values and raw text lines to SWAT configuration text
files

``` r
my_swat_writepar = function(textpath, newval=character(0), wipe=integer(0), dpmax=6)
{
  # ARGUMENTS:
  #
  # `textpath`, full path to the swat text config file
  # `newval`, named vector or list of new parameter values to overwrite the existing ones
  # `wipe`, (optional) integer vector of line numbers to replace, overrides names of `newval` 
  # `dpmax`, integer number of decimal places (maximum) to use for 'real' type variables
  #
  # RETURN VALUE:
  #
  # A data frame with columns 'written' and 'old' indicating the existing values and the ones
  # that were overwritten
  # 
  # DETAILS:
  #
  # `wipe` is used to overwrite entire lines in the text file, by specifying their line numbers.
  # Any names in `newval` are ignored in this case, and its entries must be of character type
  # (or coercible to character). If `wipe` is assigned but `newval` is not, the function simply
  # returns the text at the specified lines without overwriting anything.
  # 
  # When `wipe` is unassigned, the names of `newval` should match SWAT parameter names in the file
  # at `textpath`. The function then parses the file to overwrite the entries of `newval` in the
  # correct place. The entries of `newval` may be character, numeric or integer - they will be
  # converted to text automatically, with whitespace and truncation of decimals handled to
  # satisfy `dpmax` (whenever there is space)
  #
  # Note that the existing parameter value text in `textpath` is parsed to determine parameter
  # type and the appropriate character representation. eg. if variable 'FOO' in `textpath` is an
  # integer (with no decimal place) then `newval$FOO` will be coerced to integer, possibly by
  # rounding.
  #
  
  # whitespace separating parameters from comment delimiter
  suffix.s = strrep(' ', 4)

  # open the file and parse for SWAT variables
  text.raw = readLines(textpath)
  text.parsed = my_swat_readpar(text.raw)
  
  # unpack inputs
  names.new = names(newval)
  n.names = length(names.new)
  n.text = length(text.raw)
  n.newval = length(newval)
  n.wipe = length(wipe)
  
  # conditional for overwriting entire lines
  if(n.wipe>0)
  {
    # check for invalid lines
    if(any(wipe < 1 | wipe > n.text))
    {
      err.msg = paste0('error: valid line numbers for `wipe` are 1,...', n.text)
      stop(err.msg)
    }
    
    # check for unmatched input
    if(n.wipe != n.newval)
    {
      print(paste('error:', n.wipe, 'line numbers supplied for', n.newval, 'strings'))
    }
    
    # check for replacement text argument
    if(n.newval>0)
    {
      # copy the old text
      text.raw.new = text.raw
      
      # write the lines
      text.raw.new[wipe] = sapply(newval, as.character)
      writeLines(text.raw.new, textpath)
      
      # return the old and new lines in a data frame
      return(data.frame(old=text.raw[wipe], new=text.raw.new[wipe]))
      
    } else {
      
      # return only the old text
      return(text.raw[wipe])
      
    }
    
  # conditional for replacing parameter values
  } else {
    
    # check that we have valid names as input
    idx.parsed = match(names.new, text.parsed$name)
    if(anyNA(idx.parsed))
    {
      err.msg1 = paste('variable name(s)', missing.names, 'not found')
      err.msg2 = paste('\n', basename(textpath), 'was not modified')
      stop(err.msg1, err.msg2)
    }
    
    # grab line numbers and index special types
    newval.ln = text.parsed$line[idx.parsed]
    newval.type = text.parsed$type[idx.parsed]
    idx.num = newval.type == 'numeric'
    idx.int = newval.type == 'integer'
    idx.char = newval.type %in% c('character', 'unknown')
    
    # coerce everything to character
    newval.s = sapply(newval, as.character)
    
    # reformat special types as needed then drop leading whitespace
    newval.num = sapply(newval[idx.num], function(x) round(x, dpmax))
    newval.int = sapply(newval[idx.int], function(x) round(x, dpmax))
    newval.s[idx.num] = format(newval.num, nsmall=dpmax, scientific=FALSE)
    newval.s[idx.int] = format(newval.int, nsmall=0, scientific=FALSE)
    newval.s = gsub(' ', '', newval.s)
    
    # grab full line text from the matching lines
    linetext = setNames(text.raw[newval.ln], nm=names.new)
    
    # grab string containing the parameter with whitespace
    partxt = sapply(linetext, function(string) strsplit(string, '\\|.+')[[1]])
    
    # identify the space we have for the parameter
    par.maxn = sapply(partxt, nchar) - nchar(suffix.s)
    
    # truncate (as needed) to stay within max width
    idx.trunc = sapply(newval.s, nchar) > par.maxn
    if(any(idx.trunc))
    {
      newval.s[idx.trunc] = mapply(function(x,y) substr(x, 1, y), 
                                   newval.s[idx.trunc], 
                                   par.maxn[idx.trunc])
      
      # warn if any character strings have been truncated
      idx.truncchar = idx.trunc & idx.char
      if(any(idx.truncchar))
      {
        print(paste('warning: string(s)',
                    paste(names.new[idx.truncchar], collapse='. '), 
                    'were truncated')) 
      }
      
      # warn if truncation affected accuracy of numerics substantially
      idx.accucheck = idx.trunc & (idx.num | idx.int)
      if(any(idx.accucheck))
      {
        num.in = as.numeric(newval[idx.accucheck])
        num.out = as.numeric(newval.s[idx.accucheck])
        rel.diff = 100 * (num.in - num.out) / num.in
        if(any(rel.diff > 1))
        {
          print('warning: truncation produced relative change(s) of:')
          print(paste(paste0(rel.diff, '%'), collapse=', '))
          print(paste('in variables:', paste(names.new[idx.accucheck], collapse='. ')))
        }
      }
      
    }
    
    # pad with leading whitespace to produce strings of exactly max width
    newval.s[!idx.trunc] = mapply(function(x,y,z) paste0(strrep(' ', y-z), x),
                                  newval.s[!idx.trunc],
                                  par.maxn[!idx.trunc],
                                  nchar(newval.s[!idx.trunc]))

    
    # build replacement strings, adding terminal whitespace 
    linetext.out =  mapply(gsub, partxt, paste0(newval.s, suffix.s), linetext)
  
    # copy changes to SWAT text file
    text.raw.new = text.raw
    text.raw.new[newval.ln] = linetext.out
    writeLines(text.raw.new, textpath)
    
    # return the old and new lines in a data frame
    return(data.frame(old=text.raw[newval.ln], new=text.raw.new[newval.ln]))
    
  }
  
}
```

read/write SWAT text files for vector-valued parameter assignments

``` r
my_swat_rwvec = function(textpath, newval=character(0))
{
  # ARGUMENTS:
  #
  # `textpath`, full path to the swat text config file
  # `newval`, optional named list of (numeric or integer) vectors to overwrite the existing ones
  # 
  # RETURN VALUE:
  #
  # A (possibly empty) list of lists, one per vector-valued parameter in the SWAT file appearing
  # in `newval` or, if `newval` unassigned, the complete list of vector-valued parameters avilable
  # in the file. List entries are named according to their SWAT variable names, and contain the
  # following elements:
  #
  # ln = (integer) line number where the vector appears in the text file
  # old = (integer or numeric) vector of existing parameter values
  # new = (if specified in `newval`) the replacements for the existing values
  #
  # DETAILS:
  #
  # If `newval` is assigned, its names should match the names of the SWAT vector parameters to
  # overwrite. Note these are names for the full vector, the individual entries of which are
  # unnamed (here, and in the SWAT config files/docs). The function will overwrite the existing
  # values in the text file, handling padding and truncation appropriately
  
  # spacing info, literal strings to parse, and associated variable name for i/o control vectors
  pos.io = c(n.entries=20, len.entries=4, n.prec=0)
  key.io = c(IPDVAR = 'Reach output variables',
             IPDVAB = 'Subbasin output variables',
             IPDVAS = 'HRU output variables',
             IPDHRU = 'HRU data to be printed')
  
  # do the same for for elevation bands
  pos.el = c(n.entries=10, len.entries=8, n.prec=3)
  key.el = c(ELEVB = 'Elevation at center of elevation bands [m]',
             ELEVB_FR = 'Fraction of subbasin area within elevation band')
  
  # do the same for for climate change variables
  pos.cc = c(n.entries=6, len.entries=8, n.prec=3)
  key.cc = c(RFINC1 = 'Climate change monthly rainfall adjustment (January - June)',
             RFINC2 = 'Climate change monthly rainfall adjustment (July - December)',
             TMPINC1 = 'Climate change monthly temperature adjustment (January - June)',
             TMPINC2 = 'Climate change monthly temperature adjustment (July - December)',
             RADINC1 = 'Climate change monthly radiation adjustment (January - June)',
             RADINC2 = 'Climate change monthly radiation adjustment (July - December)',
             HUMINC1 = 'Climate change monthly humidity adjustment (January - June)',
             HUMINC2 = 'Climate change monthly humidity adjustment (July - December)')
  
  # compile everything into one data frame
  key.all = rbind(data.frame(comment=key.io, do.call(rbind, rep(list(pos.io), length(key.io)))),
                  data.frame(comment=key.el, do.call(rbind, rep(list(pos.el), length(key.el)))),
                  data.frame(comment=key.cc, do.call(rbind, rep(list(pos.cc), length(key.cc)))))
  
  # check `newval` for unmatched entries wrt the keys defined above
  newval.names = names(newval)
  idx.newval.matched = newval.names %in% rownames(key.all)
  if(sum(idx.newval.matched) != length(newval))
  {
    unmatched.char = paste(newval.names[!idx.newval.matched], collapse=', ')
    stop(paste('error: unrecognized variable(s)', unmatched.char))
  }
  
  # scan the text file for comment strings, omit non-matches, increment line number
  linetext = readLines(textpath)
  match.ln = sapply(key.all$comment,  function(pattern) grep(pattern, linetext, fixed=TRUE))
  match.ln = setNames(unlist(match.ln) + 1, rownames(key.all)[sapply(match.ln, length)>0])
  
  # finish if no matches are found
  if(length(match.ln)==0)
  { 
    # returning empty list 
    return(vector(mode='list', length=0))
    
  } 
  
  # initialize output list with line number
  out.list = lapply(match.ln, function(x) list(ln=x))
  
  # set up write-mode
  if(length(newval.names) > 0)
  {
    # check for unmatched write input variables wrt the variable names in the file
    idx.newval.matched = newval.names %in% names(match.ln)
    if(any(!idx.newval.matched))
    {
      unmatched.char = paste(newval.names[!idx.newval.matched], collapse=', ')
      err.msg = paste('error: variable(s)', unmatched.char, 'not found in', basename(textpath))
      stop(err.msg)
    }
    
    # make a copy of the full text to modify 
    linetext.new = linetext
  }
  
  # grab full text from matched line numbers
  match.linetext = setNames(linetext[match.ln], names(match.ln))
  
  # handle i/o control type vectors by pattern matching
  idx.io = names(match.linetext) %in% names(key.io)
  if(any(idx.io))
  {
    # names of the variables of this type
    io.names = names(match.linetext)[idx.io]
    
    # split at whitespace, tidy up, convert to numeric, append to output list
    io.split = strsplit(match.linetext[idx.io], '\\s+')
    io.val = lapply(io.split, function(string) as.numeric(string[sapply(string, nchar) > 0]))
    
    # append to output list
    out.list[io.names] = mapply(function(x, y) { c(x, list(old=y)) },
                                x = out.list[io.names],
                                y = io.val,
                                SIMPLIFY = FALSE)
    
    # grab the subset of `newval` to write (if any)
    idx.io.newval = io.names %in% names(newval)
    if(any(idx.io.newval))
    {
      # add zeroes to complete any missing fields of the i/o vectors
      io.newval = mapply(function(x, y) { c(x, rep(0, y-length(x))) },
                         x = newval[names(newval) %in% io.names],
                         y = key.all[io.names[idx.io.newval], 'n.entries'],
                         SIMPLIFY = FALSE)
      
      # format the numeric/integer values as character
      io.newval.s = lapply(io.newval, function(int) as.character(int))
 
      # compute amount of whitespace to lead with (if any)
      io.newval.nchar = lapply(io.newval.s, nchar)
      io.newval.nws = mapply(function(x, y) { y - x },
                                x = io.newval.nchar,
                                y = key.all[io.names[idx.io.newval], 'len.entries'],
                                SIMPLIFY = FALSE)
      
      # check for input too large to fit in specified width
      idx.newval.toobig = sapply(io.newval.nws, function(x) any(x < 0))
      toobig.names = names(which(idx.newval.toobig))
      if(any(idx.newval.toobig))
      {
        toobig.char = paste(toobig.names, collapse=', ')
        toobig.max = paste(key.all[toobig.names, 'len.entries'], collapse=', ')
        print(paste('error: invalid newval for variable(s)', toobig.char))
        print(paste('width as character exceeded upper limit(s) of', toobig.max))
      }
      
      # add leading whitespace and delimiters as needed
      io.newval.spad = mapply(function(x, y) { paste(paste0(strrep(' ', x), y), collapse='') },
                                 x = io.newval.nws,
                                 y = io.newval.s,
                                 SIMPLIFY = FALSE)
      
      # add modifications to file text and append to output list
      linetext.new[match.ln[names(io.newval.spad)]] = unlist(io.newval.spad)
      out.list[names(io.newval.spad)] = mapply(function(x, y) { c(x, list(new=y)) },
                                               x = out.list[names(io.newval.spad)],
                                               y = io.newval,
                                               SIMPLIFY = FALSE)
      
    }
  }
  
  # handle other vectors by exact character start/stop rules
  idx.other = !idx.io
  if(any(idx.other))
  {
    # names of the variables of this type
    other.names = names(match.linetext)[idx.other]
    
    # set up character start positions
    startpos = mapply(function(x, y) { 1 + x*( (1:y) - 1 ) }, 
                      x = key.all[other.names, 'len.entries'],
                      y = key.all[other.names, 'n.entries'])
    
    # set up character end positions
    endpos = mapply(function(x, y) { x + y - 1 },
                    x = key.all[other.names, 'len.entries'],
                    y = startpos)
    
    # extract the vectors as character then coerce to numeric
    other.val = mapply(function(x, y, z) { as.numeric(substring(x, y, z)) }, 
                       x = match.linetext, 
                       y = startpos, 
                       z = endpos)
    
    # append to output list
    out.list[other.names] = mapply(function(x, y) { c(x, list(old=y)) },
                                   x = out.list[other.names],
                                   y = other.val,
                                   SIMPLIFY = FALSE)
    
    # grab the subset of `newval` to write (if any)
    idx.other.newval = other.names %in% names(newval)
    if(any(idx.other.newval))
    {
      # format the numeric/integer values as character
      other.newval = newval[names(newval) %in% other.names]
      other.newval.s = lapply(other.newval, function(num) sprintf('%.3f', num))
      
      # compute amount of whitespace to lead with (if any)
      other.newval.nchar = lapply(other.newval.s, nchar)
      other.newval.nws = mapply(function(x, y) { y - x },
                                x = other.newval.nchar,
                                y = key.all[other.names[idx.other.newval], 'len.entries'],
                                SIMPLIFY = FALSE)
      
      # check for input too large to fit in specified width
      idx.newval.toobig = sapply(other.newval.nws, function(x) any(x < 0))
      toobig.names = names(which(idx.newval.toobig))
      if(any(idx.newval.toobig))
      {
        toobig.char = paste(toobig.names, collapse=', ')
        toobig.max = paste(key.all[toobig.names, 'len.entries'], collapse=', ')
        print(paste('error: invalid newval for variable(s)', toobig.char))
        print(paste('width as character exceeded upper limit(s) of', toobig.max))
      }
      
      # add leading whitespace as needed
      other.newval.spad = mapply(function(x, y) { paste(paste0(strrep(' ', x), y), collapse='') },
                                 x = other.newval.nws,
                                 y = other.newval.s,
                                 SIMPLIFY = FALSE)
      
      # add modifications to file text and build output list
      linetext.new[match.ln[names(other.newval.spad)]] = unlist(other.newval.spad)
      out.list[names(other.newval.spad)] = mapply(function(x, y) { c(x, list(new=y)) },
                                                  x = out.list[names(other.newval.spad)],
                                                  y = other.newval,
                                                  SIMPLIFY = FALSE)
      
    }
    
  }
  
  # write the changes (if any) to the file 
  if(length(newval.names) > 0)
  {
    writeLines(linetext.new, textpath)
    out.list = out.list[newval.names]
  }
  
  # return list of current and new parameter values
  return(out.list)
}
```

return a list of SWAT text input files and associated variable names

``` r
my_swat_scan = function(textio.dir)
{
  # ARGUMENTS:
  #
  # `textio.dir`, character string path to the text input file directory of a SWAT2012 model
  #
  # RETURN VALUE:
  #
  # A list of all text input configuration files for the SWAT model (all located in the `textio.dir`),
  # and the SWAT variable names available for reading/writing. List entries are named according
  # to the file extension (eg. cio, bsn, etc), and are themselves lists of named character vectors:
  # 
  #  file = the filename(s) associated to this extension
  #  link = external filename(s) referenced in this/these files
  #  scalar = SWAT variable names for single-valued parameters listed in this file
  #  vector = SWAT variable names for vector-valued parameters listed in this file
  #
  # Note that 'link' and 'vector' are included only for relevant files (ie where they are nonempty)
  # 
  # DETAILS:
  #
  # The first two entries of the return list ('cio' and 'bsn') are global parameter files, which
  # apply to all subbasins equally. The remainder ('sub', 'hru', 'mgt', 'gw', 'sep'), refer to a set
  # of files (one per subbasin), which share identically named parameters that can vary at the subbasin
  # level. These parameters are only listed once (rather than repeated for each subbasin)
  #
  
  # scan all existing files in the directory 
  all.fn = list.files(textio.dir)
  
  ## master watershed file ('file.cio')
  
  # open 'file.cio' and parse for standard SWAT variables
  cio.path = file.path(textio.dir, 'file.cio')
  cio.raw = readLines(cio.path, warn=FALSE)
  cio.df = my_swat_readpar(cio.raw)
  
  # pull file references (ignoring unassigned ones) and numeric/integer variable names
  idx.cio.char = (cio.df$type=='character') & (cio.df$value != '')
  cio.fn.standard = setNames(cio.df$value[idx.cio.char], cio.df$name[idx.cio.char])
  
  # define literal strings that precede some special filename entries
  cio.lb = c(fig='Watershed Configuration',
             pcp='Precipitation Files', 
             tmp='Temperature Files',
             atm='ATMOSPERIC DEPOSITION') # <- this typo intentional!
  
  # find these strings and parse the subsequent line number
  cio.ln = apply(sapply(cio.lb, function(pattern) grepl(pattern, cio.raw)), 2, which)
  cio.fn.special = setNames(gsub(' ', '', cio.raw[1+cio.ln]), nm=names(cio.ln))
  
  # pull all remaining scalar-valued variable names
  cio.scalar = cio.df$name[!idx.cio.char]
  
  # pull vector-valued parameter names
  cio.vector = names(my_swat_rwvec(cio.path))
  
  ## descendants (.bsn, .fig)
  
  # open the open basin input (.bsn) file and pull scalars, file references (ignoring unassigned ones)
  bsn.path = file.path(textio.dir, cio.fn.standard['BSNFILE'])
  bsn.raw = readLines(bsn.path, warn=FALSE)
  bsn.df = my_swat_readpar(bsn.raw)
  bsn.char.idx = (bsn.df$type=='character') & (bsn.df$value != '')
  bsn.fn.standard = setNames(bsn.df$value[bsn.char.idx], bsn.df$name[bsn.char.idx])
  bsn.scalar = bsn.df$name[!bsn.char.idx]
  
  # open watershed configuration file (.fig) and pull file references
  fig.raw = readLines(file.path(textio.dir, cio.fn.special['fig']), warn=FALSE)
  fig.ln = which(grepl('Subbasin: [1-9]+', fig.raw))
  fig.id = sapply(strsplit(fig.raw[fig.ln], 'Subbasin:'), function(x) as.integer(x[length(x)]))
  fig.fn = setNames(gsub(' ', '', fig.raw[fig.ln + 1], fixed=TRUE), paste0('sub_', fig.id))
  sub.n = length(fig.fn)
  
  ## scan sub-watershed level files (`sub.n` of them)
  
  # in addition to parameters in the .sub files, some are found in linked files 
  sub.special.toread = c('hru'=T, 'mgt'=T, 'sol'=F, 'chm'=F, 'gw'=T, 'sep'=T)
  
  # this pattern grabs the linked filenames
  sub.regexp = paste(paste0('.', names(sub.special.toread), '\\s*'), collapse='|')
  
  # open subbasin files (.sub) in a loop, writing file details into list
  sub.fn.list = vector(mode='list', length=sub.n) 
  for(idx.sub in 1:sub.n)
  {
    # pull file references
    sub.raw = readLines(file.path(textio.dir, fig.fn[idx.sub]), warn=FALSE)
    sub.df =  my_swat_readpar(sub.raw)
    sub.char.idx = (sub.df$type=='character') & (sub.df$value != '')
    sub.fn.standard = setNames(sub.df$value[sub.char.idx], sub.df$name[sub.char.idx])
    
    # parse special file references
    sub.special.raw = sub.raw[1+which(grepl('HRU: General', sub.raw))]
    sub.fn.special = paste(strsplit(sub.special.raw, sub.regexp)[[1]], names(sub.special.toread), sep='.')
    names(sub.fn.special) = names(sub.special.toread)
    
    # copy filenames to list
    sub.fn.list[[idx.sub]] = c(sub.fn.special, sub.fn.standard)
    
  }
  
  # flatten list into data frame, appending 'sub' column
  sub.fn.all = cbind(sub=fig.fn, data.frame(do.call(rbind, sub.fn.list), row.names=names(fig.fn)))
  
  # open files corresponding to first subbasin to get scalars (they should all be the same)
  ext.toread = c('sub', names(sub.special.toread)[sub.special.toread])
  sub.scalar.list = setNames(vector(mode='list', length=length(ext.toread)), ext.toread) 
  for(ext in ext.toread)
  {
    # scan for SWAT variable names and copy them to the list
    sub.path = file.path(textio.dir, sub.fn.all[1, ext])
    sub.df = my_swat_readpar(readLines(sub.path, warn=FALSE))
    sub.char.idx = (sub.df$type=='character') & (sub.df$value != '')
    sub.scalar.list[[ext]] = sub.df$name[!sub.char.idx]

  }
  
  # organize all sub-watershed files and parameter names into list
  out.sub = lapply(setNames(nm=ext.toread), function(ext) {
    list(file=setNames(sub.fn.all[,ext], rownames(sub.fn.all)), 
         scalar=sub.scalar.list[[ext]])
  })
  
  
  # pull vector-valued parameter names from .sub files and add to list
  out.sub$sub$vector = names(my_swat_rwvec(file.path(textio.dir, sub.fn.all[1, 'sub'])))
  
  # compile everything for cio 
  out.cio = list(cio=list(file='file.cio', 
                          link=c(cio.fn.standard, cio.fn.special), 
                          scalar=cio.scalar,
                          vector=cio.vector))
  
  # compile everything for bsn
  out.bsn = list(bsn=list(file=cio.fn.standard['BSNFILE'], 
                          link=bsn.fn.standard, 
                          scalar=bsn.scalar))
  
  # finish
  return(c(out.cio, out.bsn, out.sub))
  
}
```

read/write SWAT configuration text file parameters

``` r
my_swat_rw = function(textio.dir, param=character(0), textio.scan=list(), subb=integer(0))
{
  # ARGUMENTS:
  #
  # `textio.dir`, character string path to the text input file directory of a SWAT2012 model
  # `param`, vector (or list) of either SWAT variable names (character), or numeric values to assign
  # `textio.scan`, (optional) list, the result of `my_swat_scan(textio.dir)`
  # `subb`, (optional) integer vector of subbasin IDs
  #
  # RETURN VALUE:
  #
  # ...
  # 
  # DETAILS:
  #
  # ...
  # 
  
  # examples 
  # my_swat_rw(textio.dir)
  # my_swat_rw(textio.dir, param='IDAL')
  # my_swat_rw(textio.dir, param=c(IDAL=NA))
  # my_swat_rw(textio.dir, param='LAT_ORGN')
  # my_swat_rw(textio.dir, param=c('IDAL', 'LAT_ORGN'))
  # my_swat_rw(textio.dir, param=c('LAT_ORGN'=1))
  # my_swat_rw(textio.dir, param=c(IDAL=273, LAT_ORGN=0))
  # my_swat_rw(textio.dir, param='IPDVAR')
  # my_swat_rw(textio.dir, param=list(IPDVAR=c(3,5,10,15)))
  # my_swat_rw(textio.dir, param=list(IPDVAR=c(1,2)))
  # my_swat_rw(textio.dir, param='RFINC1')
  # my_swat_rw(textio.dir, param=list(RFINC1=c(0,0,0,0,0,0)))
  
  # this argument can be used to prevent repetitive file reads
  if(length(textio.scan)==0)
  {
    # scan directory and read in all config files
    textio.scan = my_swat_scan(textio.dir)
  } 
  
  # make a vector of available variable names
  all.parnames = unlist(sapply(textio.scan, function(cfg) c(cfg$scalar, cfg$vector)), use.names=F)
  
  # if `param` is unassigned, return a data frame summarizing available SWAT parameters
  if(length(param)==0)
  {
    # print message and return available variable names
    print('`param` argument specifies the SWAT parameter(s) to read/write') 
    return(sort(all.parnames))
  }
  
  # if `param` is unnamed, it is treated as a vector/list of parameter names to query
  input.names = names(param)
  if(is.null(input.names))
  {
    # call the function again with `param` a named list of NAs to prompt read-mode
    param.new = setNames(as.list(rep(NA, length(param))), unlist(param))
    return(my_swat_rw(textio.dir, param=param.new, textio.scan=textio.scan, subb=subb))
    
  }
  
  # convert vectors to lists
  param = as.list(param)
  input.names = names(param)
  
  # check that names of `param` are all valid SWAT parameter names before proceeding
  idx.valid = input.names %in% all.parnames
  if(!all(idx.valid))
  {
    err.msg1 = 'error: failed to match element(s) of input `param` to SWAT variables: '
    err.msg2 = paste(input.names[!idx.valid], collapse=', ')
    stop(err.msg1, err.msg2)
    
  }
  
  # handle `param` lists with more than one element using recursive call
  if(length(param)>1)
  {
    # apply the function to each list element in `param` using lapply
    out.list = lapply(seq_along(param), function(idx) {
      param.singleton = setNames(param[idx], input.names[idx]) 
      my_swat_rw(textio.dir, param=param.singleton, textio.scan=textio.scan, subb=subb)
      })
    
    # finish with list output
    return(setNames(out.list, input.names))
  
  }
  
  # `param` will be of length 1 if we have gotten this far: find its type
  idx.fn.scalar = sapply(textio.scan, function(cfg) input.names %in% cfg$scalar)
  idx.fn.vector = sapply(textio.scan, function(cfg) input.names %in% cfg$vector)
  is.scalar = any(idx.fn.scalar)
  
  # build path to container file(s) - always prefer subbasin-level files (which have priority)
  idx.fn = which(idx.fn.scalar | idx.fn.vector)
  if(length(idx.fn) == 2)
  {
    idx.fn = idx.fn[-1]
  }
  fn = textio.scan[[idx.fn]]$file
  textio.file = file.path(textio.dir, fn)
  
  # check if parameter is global or subbasin-level (appearing in multiple files)
  is.global = length(textio.file) == 1
  
  # when a subset of subbasins is specified, only scan the associated subset of files
  if(length(subb) > 0 & !is.global)
  {
    # extract the subbasin IDs
    subb.all = as.integer(gsub('sub_', '', names(fn)))
    
    # check for unmatched input subbasin IDs
    idx.subb.valid = subb %in% subb.all
    if(any(!idx.subb.valid))
    {
      err.msg = paste('error: subbasin IDs', paste(subb[idx.subb.valid], collapse=', '), 'not found')
      stop(err.msg)
    }
    
    # take the subset
    idx.subb = match(subb, subb.all)
    fn = fn[idx.subb]
    textio.file = textio.file[idx.subb]
  }

  # non-vector and vector cases are handled by separate helper functions
  if(is.scalar)
  {
    # read mode:
    if(is.na(unlist(param)))
    {
      # read in all parameter values (from all selected files)
      textio.df = lapply(textio.file, function(x) my_swat_readpar(readLines(x, warn=FALSE)))
      
      # extract the requested parameter(s)
      param.idx = textio.df[[1]]$name == input.names
      param.type = textio.df[[1]]$type[param.idx]
      param.out = as(sapply(textio.df, function(df) df$value[param.idx]), param.type)
      
      # if returning from multiple subbasins, name according to subbasin ID
      if(!is.global)
      {
        names(param.out) = names(fn) 
        
      } else {
        
        # otherwise set name to SWAT variable name
        names(param.out) = input.names 
      }
      
      # return the requested parameters 
      return(param.out)
      
      
    } else {
      # write mode:
      
      # write mode for global simply calls another helper function
      if(is.global)
      {
        return(my_swat_writepar(textio.file, newval=unlist(param)))
        
      } else {
        
        out.list = lapply(textio.file, function(x) my_swat_writepar(x, newval=unlist(param)))
        return(setNames(out.list, names(fn)))
      }
    }
    
  } else {
    # handle vector-valued parameters
    
    # read mode:
    if(anyNA(unlist(param)))
    {
      # read in all vector-valued parameters using a helper function
      param.out = lapply(textio.file, function(x) my_swat_rwvec(x)[[input.names]]$old)
      
      # if returning from multiple subbasins, name according to subbasin ID
      if(!is.global)
      {
        names(param.out) = names(fn) 
        
      } else {
        
        # otherwise set name to SWAT variable name
        names(param.out) = input.names 
      }
      
      # return the requested parameters 
      return(param.out)
      
    } else {
      # write mode:
      
      # write mode for global simply calls another helper function
      if(is.global)
      {
        out.rwvec = my_swat_rwvec(textio.file, newval=param)
        out.df = data.frame(do.call(rbind, out.rwvec[[1]][c('old', 'new')]))
        return(setNames(list(out.df), input.names))
        
      } else {
        
        out.rwvec = lapply(textio.file, function(x) my_swat_rwvec(x, newval=param))
        out.list = lapply(out.rwvec, function(x) data.frame(do.call(rbind, x[[1]][c('old', 'new')]))) 
        return(setNames(out.list, names(fn)))
      }
    }
    
  }
  
  #print(input.names)
  
  
  # # If the value of `param[[1]]` is NA, we are in read-mode
  # if(!is.na(param[[1]]))
  # {
  #   # coerce to list and call the function again
  #   return(my_swat_rw(textio.dir, as.list(param), textio.scan=textio.scan))
  # }
  
  
  
}
```

write weather input text files for QSWAT and SWAT2012

``` r
my_swat_wmeteo = function(wdat, exdir, form='qswat', include=logical(0), suffix='')
{
  # ARGUMENTS:
  #
  # `wdat`: list of time series data: 'coords_sf', 'dates', 'tables', 'elevation' (see DETAILS) 
  # `exdir`: character string, path to directory to write output files
  # `form` : character string specifying output structure, either 'qswat' (the default) or 'swat'
  # `include`: (optional) boolean vector of same length as `wdat$coords_sf`, for writing subsets
  # `suffix`: (optional) suffix to append to filenames
  #
  #
  # RETURN VALUE:
  #
  # A list of all filenames written and their directory (`exdir`)
  #
  #
  # DETAILS: 
  #
  # Writes one of two different file structures, depending on `form`:
  #
  # For `swat`, we write one file per SWAT variable found in `wdat`. This contains (lat/long)
  # coordinates and the full time series of all point data sources (eg. 'pcp1.pcp' contains
  # precipitation data from all locations, as a numeric matrix). Elevation is also written to
  # these files (as integer) whenever it is supplied in `wdat`. Note, however, that the elevations
  # stored in these files are ignored by SWAT, which bases any elevation-related calculations on
  # the subbasin elevations listed in the *.sub files. Similarly, SWAT maps point weather data
  # sources to subbasins based on the integer IDs listed in the *.sub files, and not the lat/long
  # values in the weather input files (eg. integer ID "23" refers to the 23rd column of 'pcp1.pcp')
  #
  # With 'qswat', the point data location info DOES matter (it is used by QSWAT to construct the
  # *.sub files), and is stored separately from the actual time series data: Each weather variable
  # has a single text file (eg. 'pcp.txt') that lists the point coordinates and their elevations;
  # whereas the time series data go into location-specific files (eg. 'tmp_grid23.txt'). It's not
  # clear to me whether the elevation data are important in the 'qswat' case, but you should
  # probably include them to be on the safe side.
  #
  # The required entries of `wdat` are:
  #
  #   coords_sf: sfc POINT object with `name` attribute (unique names for each point data source)
  #   dates: vector of Date objects, in same order as the rows of the weather tables
  #   tables: named list of dataframes, one per variable, where column names match coords_sf$name
  #   elevation: (optional) named list of numeric elevations (in metres)
  # 
  # Supported SWAT variable names are: 
  # 
  # 'pcp' (precip, in mm/day)
  # 'tmin' and 'tmax' (temperature in degrees Celsius, where both min and max must be specified)
  # 'slr' (daily solar radiation, in MJ/m2)
  # 'wnd' (average speed, in m/s)
  # 'hmd' (relative humidity, expressed as fraction)
  # 
  # Entries of `wdat$tables` that do not match these names are ignored (nothing written). Note
  # that since 'tmin' and 'tmax' appear in the same SWAT input file ('.tmp'), they must both be
  # supplied or the function will write neither.
  #
  # Output filenames in 'swat' mode have the form '<varname><suffix>.<varname>' (eg. with
  # `suffix=1`, the precip data is written to the file 'pcp1.pcp'). In 'qswat' mode, they have the
  # form '<varname>.txt' for location data, and '<varname><suffix>_<pointname>.txt' for time series
  # data (eg. 'pcp1.txt' contains the location info for time series files 'pcp1_grid1.txt',
  # 'pcp1_grid2.txt', etc). The only exception is for temperature, where there is a single '.tmp'
  # file instead of separate tmin and tmax files.
  #
  # The optional `include` vector specifies (as boolean index) a subset of `wdat$coords_sf` to
  # include in the output. Note that for 'swat' mode, this controls the number of columns in the
  # output, and therefore the mapping (of columns to subbasins) in *.sub files should be adjusted
  # accordingly. By default all the data are written.
  #
  # It seems that SWAT+ Editor will fail to 
  #
  
  # missing data field (NA) is coded as "-99.0"
  na.value = -99
  
  # set EPSG code for latitude/longitude
  epsg.geo = 4326
  
  # define SWAT variable names, and expected input variable names
  vn.list = list(pcp='pcp', slr='slr', wnd='wnd', hmd='hmd', tmp=c('tmax', 'tmin'))
  
  # first date in the time series
  origin.date = wdat$dates[1]
  
  # check for invalid `form` argument
  if(!(form %in% c('qswat', 'swat')))
  {
    stop('`form` must either be "qswat" or "swat"') 
  }
  
  # if elevations not supplied, use missing data value
  if(is.null(wdat$elevation))
  {
    wdat$elevation =  setNames(rep(na.value, nrow(wdat$coords_sf)), wdat$coords_sf$name)
  }
  
  # handle unassigned `include` argument
  if(length(include)==0)
  {
    # default behaviour is to write all points
    include = setNames(rep(TRUE, nrow(wdat$coords_sf)), wdat$coords_sf$name)
  }
  
  # build index of `wdat` to write, extract geographic coordinates
  coords = wdat$coords_sf[include,]
  coords.geo = st_coordinates(st_transform(coords, epsg.geo))
  n.coords = nrow(coords)
  
  # contruct table of geographic (lat/long) coordinates, append elevations
  coords.tab = setNames(data.frame(coords.geo, row.names=coords$name), c('long', 'lat'))
  coords.tab$elevation = wdat$elevation[include]
  
  # it's unclear if there is a limit on precision - 3 decimal places should be plenty
  coords.tab = round(coords.tab, 3)
  
  # check for input matching SWAT variable names
  vn.idx = sapply(vn.list, function(nm) all(nm %in% names(wdat$tables)))
  vn.in = names(vn.idx)[vn.idx]
  
  # warn of unpaired temperature input (one but not both of 'tmin' or 'tmax' supplied)
  if(!vn.idx['tmp'] & any(c('tmin', 'tmax') %in% names(wdat$tables)))
  {
    warning('one of tmin/tmax not supplied. Temperature data will not be written')
  }
  
  # construct a list of strings to use as station-variable names (filenames, in 'qswat' mode)
  svn = setNames(lapply(vn.in, function(vn) paste(paste0(vn, suffix), coords$name, sep='_')), vn.in)
  
  # set up output directory (creates it if it doesn't exist)
  my_dir(exdir)
  
  # replace NA fields with numeric flag for missing data
  wdat$tables = lapply(wdat$tables, function(xx) xx %>% replace(is.na(.), na.value))

  # generate SWAT-readable text input files
  if(tolower(form)=='swat')
  {
    # Note that the first 4 lines of these files are treated as comments by SWAT
    l2.string = paste(c('Lati  ', substring(as.character(coords.tab[['lat']]), 1, 4)), collapse=' ')
    l3.string = paste(c('Long  ', substring(as.character(coords.tab[['long']]), 1, 4)), collapse=' ')
    l4.string = paste(c('Elev  ', substring(as.character(coords.tab[['elev']]), 1, 4)), collapse=' ')
    
    # define paths to the output files (one per variable)
    wstn.path = sapply(vn.in, function(vn) file.path(exdir, paste0(vn, suffix, '.txt')))
    
    # prepare Julian date-year strings that start each line
    dates.string = format(wdat$dates[include], '%Y%j')
    
    # write the variables one at a time in a loop
    pb = txtProgressBar(max=length(vn.in), style=3)
    for(idx.vn in 1:length(vn.in))
    {
      # print name of the variable to write
      vn = vn.in[idx.vn]
      l1.string = paste('Station ', paste(svn[[vn]], collapse=','))
      print(paste('writing', basename(wstn.path[vn]), 'to directory', exdir))
      
      # handle temperature data, which requires concatentation of two variables
      if(vn=='tmp')
      {
        # we get 10 characters total to write the minmax vals (with no delimiters), or 5 per extremum
        tsmin.matrix = sapply(wdat$tables[['tmin']][, include], function(xx) sprintf(xx, fmt='%05.1f'))
        tsmax.matrix = sapply(wdat$tables[['tmax']][, include], function(xx) sprintf(xx, fmt='%05.1f'))
        
        # concatenate the max and min (entrywise), then concatenate lines
        ts.matrix = t(sapply(1:nrow(tsmin.matrix), function(idx) paste0(tsmax.matrix[idx,], tsmin.matrix[idx,])))
        ts.out = t(apply(ts.matrix, 1, paste0))
        
      } else {
        
        # handle standard variables, creating matrix of values, then concatenating by line
        ts.matrix = sapply(wdat$tables[[vn]][, include], function(xx) sprintf(xx, fmt='%05.1f'))
        ts.out = t(apply(ts.matrix, 1, paste0))
        
      }
      
      # append dates to beginning of each line of numeric data 
      ts.lines = sapply(1:nrow(ts.out), function(idx) paste(c(dates.string[idx], ts.out[idx,]), collapse=''))
      
      # append comment lines and write to disk
      writeLines(c(l1.string, l2.string, l3.string, l4.string, ts.lines), con=wstn.path[vn])
      setTxtProgressBar(pb, idx.vn)
      
    }
    close(pb) 
    
    # finish and return filenames in list
    return(list(exdir=exdir,
                stations=setNames(basename(wstn.path), vn.in)))
    
  }
  
  # generate QSWAT-readable text input files
  if(tolower(form)=='qswat')
  {
    # build wdat station file tables for QSWAT as matrices of text
    wdat.wstn = lapply(setNames(nm=vn.in), function(vn) { 
      cbind(ID=1:n.coords, 
            NAME=svn[[vn]], 
            LAT=unname(coords.tab['lat']),
            LONG=unname(coords.tab['long']),
            ELEVATION=unname(coords.tab['elevation']))
    })

    # define paths to the output files (with time series stored separately from station location info)
    wstn.path = sapply(names(svn), function(fn) file.path(exdir, paste0(fn, suffix, '.txt')))
    wstn.ts.path = lapply(svn, function(fn) setNames(file.path(exdir, paste0(fn, '.txt')), nm=coords$name))
    
    # write the station location data
    sapply(names(svn), function(vn) write.csv(wdat.wstn[[vn]], file=wstn.path[vn], quote=F, row.names=F))
    
    # the first line of each time series data file is the origin date (without spaces) 
    origin.string = paste0(gsub('-', '', origin.date))
    
    # write the station time series data in a loop
    pb = txtProgressBar(max=length(vn.in)*n.coords, style=3)
    for(idx.vn in 1:length(vn.in))
    {
      # print name of the variable to write
      vn = vn.in[idx.vn]
      print(paste('writing', n.coords, vn, 'files to directory', exdir))
      
      # loop over grid point locations
      for(idx.coords in 1:n.coords)
      {
        # identify point name and output path for the text file
        stn.name = coords$name[idx.coords]
        out.path = wstn.ts.path[[vn]][stn.name]
        setTxtProgressBar(pb, idx.coords + (idx.vn-1)*n.coords)
        
        # handle temperature data, which requires concatentation of two variables
        if(vn=='tmp')
        {
          # write txt containing comma-separated concatenation of variables by row
          ts.out = sapply(wdat$tables[vn.list[['tmp']]], function(dat) dat[[stn.name]])
          writeLines(c(origin.string, apply(ts.out, 1, paste, collapse=',')), con=out.path)
          
        } else {
          
          # write txt for usual case of a single variable in each file (eg. prec, wind)
          ts.out = wdat$tables[[vn.list[[vn]]]][[stn.name]]
          writeLines(c(origin.string, ts.out), con=out.path)
          
        }
        
      }
      
    }
    close(pb) 
    
    # finish and return filenames in list
    return(list(exdir=exdir,
                stations=setNames(basename(wstn.path), vn.in), 
                data=lapply(wstn.ts.path, basename)))
  }
  
}
```

compute Nash–Sutcliffe model efficiency coefficient (NSE)

``` r
my_nse = function(qobs, qsim, L=2, normalized=FALSE)
{
  # compute the standard NSE coefficient
  nse = 1 - ( sum( abs(qsim - qobs)^L ) / sum( abs(qobs - mean(qobs))^L ) )
  
  # normalize, if requested
  if(normalized)
  {
    nse = 1 / (2 - nse)
  }
  
  return(nse)
}

# # print all available SWAT+ plant codes containing the following keywords
# kw = 'forest|woodland|pine'
# plants_plt %>% filter(grepl(kw, description, ignore.case=TRUE)) %>% pull(description)
# 
# # define some keywords to parse the NVC categories ('' means show all results at this level)
# kwdiv = 'forest|woodland'
# kwmacro = ''
# kwgroup = 'pine'
# 
# # build a copy of the land use table with explicit tree structure in column `pathString`
# landuse.tree = landuse.tab %>%
#   filter(grepl(kwdiv, NVC_DIV, ignore.case=TRUE)) %>%
#   filter(grepl(kwmacro, NVC_MACRO, ignore.case=TRUE)) %>%
#   filter(grepl(kwgroup, NVC_GROUP, ignore.case=TRUE)) %>%
#   mutate(pathString=paste('NVC', NVC_DIV, NVC_MACRO, NVC_GROUP, sep='/'))
# 
# # print tree structure, displaying the number of pixels and any existing assignments to SWAT+ plant codes
# print(as.Node(landuse.tree), 'n_uyrw', 'swatdesc', limit=125)
```
